{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a519495-7b12-4c81-97ec-d5888b6121ae",
   "metadata": {},
   "source": [
    "### 1. 读取原始数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a828c837-1cc7-49c1-b5cd-5e38ce6e99ad",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'opencc_clib' from 'opencc.clib' (D:\\software\\Anaconda\\envs\\py39_normal\\lib\\site-packages\\opencc\\clib\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32mD:\\software\\Anaconda\\envs\\py39_normal\\lib\\site-packages\\opencc\\__init__.py:7\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m----> 7\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mopencc_clib\u001b[39;00m               \n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'opencc_clib'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mjoblib\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mjieba\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mopencc\u001b[39;00m\n",
      "File \u001b[1;32mD:\\software\\Anaconda\\envs\\py39_normal\\lib\\site-packages\\opencc\\__init__.py:9\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mopencc_clib\u001b[39;00m               \n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m----> 9\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopencc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m opencc_clib\n\u001b[0;32m     11\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOpenCC\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCONFIGS\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__version__\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     13\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m opencc_clib\u001b[38;5;241m.\u001b[39m__version__\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'opencc_clib' from 'opencc.clib' (D:\\software\\Anaconda\\envs\\py39_normal\\lib\\site-packages\\opencc\\clib\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import os, joblib, jieba, opencc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18c7164-9af5-4137-9a0a-57983875d979",
   "metadata": {},
   "source": [
    "### 2. 构建分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04d8037-12c5-4a5e-a620-bf1a0fd8c472",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer(object):\n",
    "    \"\"\"\n",
    "        自定义一个分词器，实现基本功能：\n",
    "            - 1. 根据输入的语料，构建字典\n",
    "            - 2. 输入src的句子，输出对应的id\n",
    "            - 3. 输入tgt的句子，输出对应的id\n",
    "            - 4. 输入tgt的id，输出tgt的句子\n",
    "    \"\"\"\n",
    "    def __init__(self, data_file):\n",
    "        \"\"\"\n",
    "            分词器初始化\n",
    "                - 默认：根据输入的语料，构建字典\n",
    "        \"\"\"\n",
    "        self.data_file = data_file\n",
    "        self.data = None\n",
    "        self.src_token2idx = None\n",
    "        self.src_idx2token = None\n",
    "        self.tgt_token2idx = None\n",
    "        self.tgt_idx2token = None\n",
    "        self._build_dict()\n",
    "    \n",
    "    def _build_dict(self):\n",
    "        \"\"\"\n",
    "            构建字典\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.src_token2idx:\n",
    "            print(\"字典已经构建过了\")\n",
    "            return\n",
    "        elif os.path.exists(os.path.join(\".cache\", \"dicts.lxh\")):\n",
    "            print(\"从缓存中读取字典\")\n",
    "            self.src_token2idx, self.src_idx2token, self.tgt_token2idx, self.tgt_idx2token = joblib.load(filename=os.path.join(\".cache\", \"dicts.lxh\"))\n",
    "            return\n",
    "        \n",
    "        # 从零构建字典\n",
    "        self.data = pd.read_csv(filepath_or_buffer=self.data_file, sep=\"\\t\", names=[\"src\", \"tgt\"])\n",
    "        # self.data.columns = [\"src\", \"tgt\"]\n",
    "        rows, cols  = self.data.shape\n",
    "        # 构建词典\n",
    "        src_tokens = {\"<UNK>\", \"<PAD>\", \"<SOS>\", \"EOS\"}\n",
    "        tgt_tokens = {\"<UNK>\", \"<PAD>\", \"<SOS>\", \"EOS\"}\n",
    "        for row_idx in tqdm(range(rows)):\n",
    "            src, tgt = self.data.loc[row_idx, :]\n",
    "            src_tokens.update(set(self.split_english_sentence(src)))\n",
    "            tgt_tokens.update(set(self.split_chinese_sentence(tgt)))\n",
    "        # 构建 src 的 字典\n",
    "        self.src_token2idx = {token: idx for idx, token in enumerate(src_tokens)}\n",
    "        self.src_idx2token = {idx: token for token, idx in self.src_token2idx.items()}\n",
    "\n",
    "        # 构建 tgt 的 字典\n",
    "        self.tgt_token2idx = {token: idx for idx, token in enumerate(tgt_tokens)}\n",
    "        self.tgt_idx2token = {idx: token for token, idx in self.tgt_token2idx.items()}\n",
    "\n",
    "        # 保存\n",
    "        dicts = [self.src_token2idx, self.src_idx2token, self.tgt_token2idx, self.tgt_idx2token]\n",
    "        joblib.dump(value=dicts, filename=os.path.join(\".cache\", \"dicts.lxh\"))\n",
    "        \n",
    "    def split_english_sentence(self, sentecne):\n",
    "        \"\"\"\n",
    "            英文句子切分\n",
    "        \"\"\"\n",
    "        sentecne = sentecne.strip()\n",
    "        tokens = [word for word in jieba.lcut(sentecne.lower()) if word not in (\"\", \" \", \"'\")]\n",
    "        return tokens\n",
    "    \n",
    "    def split_chinese_sentence(self, sentence):\n",
    "        \"\"\"\n",
    "            中文句子切分\n",
    "        \"\"\"\n",
    "        # 实例化一个繁体转简体的工具\n",
    "        converter = opencc.OpenCC(config=\"t2s\")\n",
    "        sentence = converter.convert(text=sentence)\n",
    "        # 分词\n",
    "        tokens = jieba.lcut(sentence)\n",
    "        return tokens\n",
    "        \n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "            返回必要的打印信息\n",
    "        \"\"\"\n",
    "        if self.src_token2idx:\n",
    "            out = f\"Tokenizer: [src: {len(self.src_token2idx)}, tgt: {len(self.tgt_token2idx)}]\"\n",
    "        else:\n",
    "            out = f\"尚无字典信息\"\n",
    "        return out\n",
    "    \n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "            返回必要的打印信息\n",
    "        \"\"\"\n",
    "        return self.__str__()\n",
    "\n",
    "    def encode_src(self, src_sentence, src_max_len):\n",
    "        \"\"\"\n",
    "            把分词后的句子，变成 id\n",
    "        \"\"\"\n",
    "        src_idx = [self.src_token2idx.get(token, self.src_token2idx.get(\"<UNK>\")) for token in src_sentence]\n",
    "        src_idx = (src_idx + [self.src_token2idx.get(\"<PAD>\")] * src_max_len)[:src_max_len]\n",
    "        return src_idx\n",
    "\n",
    "    def encode_tgt(self, tgt_sentence, tgt_max_len):\n",
    "        \"\"\"\n",
    "            把分词后的tgt句子变成 id\n",
    "                - <SOS>, 我, 爱, 北京, 天安门, ！, <EOS>, <PAD>, <PAD>\n",
    "        \"\"\"\n",
    "        tgt_sentence = [\"SOS\"] + tgt_sentence + [\"EOS\"]\n",
    "        tgt_max_len += 2\n",
    "        tgt_idx = [self.tgt_token2idx.get(token, self.tgt_token2idx.get(\"<UNK>\")) for token in tgt_sentence]\n",
    "        tgt_idx = (tgt_idx + [self.tgt_token2idx.get(\"<PAD>\")] * tgt_max_len)[:tgt_max_len]\n",
    "        return tgt_idx       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bb4edd-0aa1-4136-878d-f96607af30d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64092dda-96ab-4f86-aabb-945f2bad26cf",
   "metadata": {},
   "source": [
    "### 3. 数据打包\n",
    "- 既要又要\n",
    "    - 既要批量化训练\n",
    "    - 又要消除填充PAD的噪声污染\n",
    "- collate_fn\n",
    "    - 手动排序！！！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e34ac30c-7758-46b0-b479-633dbb5e246b",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 126] 找不到指定的模块。 Error loading \"D:\\software\\Anaconda\\envs\\py39_normal\\lib\\site-packages\\torch\\lib\\torch_python.dll\" or one of its dependencies.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n",
      "File \u001b[1;32mD:\\software\\Anaconda\\envs\\py39_normal\\lib\\site-packages\\torch\\__init__.py:148\u001b[0m\n\u001b[0;32m    146\u001b[0m                 err \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mWinError(ctypes\u001b[38;5;241m.\u001b[39mget_last_error())\n\u001b[0;32m    147\u001b[0m                 err\u001b[38;5;241m.\u001b[39mstrerror \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m Error loading \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdll\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m or one of its dependencies.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 148\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m    150\u001b[0m     kernel32\u001b[38;5;241m.\u001b[39mSetErrorMode(prev_error_mode)\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_preload_cuda_deps\u001b[39m(lib_folder, lib_name):\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 126] 找不到指定的模块。 Error loading \"D:\\software\\Anaconda\\envs\\py39_normal\\lib\\site-packages\\torch\\lib\\torch_python.dll\" or one of its dependencies."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import pandas\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b8358a9-f212-4362-8213-26a3b12110b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "从缓存中读取字典\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(data_file=\"data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89bd09fa-f86b-44d7-b1dc-096674c80de4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tokenizer: [src: 7106, tgt: 12547]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b501b407-b66d-4ad3-a2cc-48f86120a9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqDataset(Dataset):\n",
    "    \"\"\"\n",
    "        自定义数据集\n",
    "    \"\"\"\n",
    "    def __init__(self, data_file, part=\"train\", tokenizer=tokenizer):\n",
    "        \"\"\"\n",
    "            初始化\n",
    "        \"\"\"\n",
    "        self.data_file = data_file\n",
    "        self.tokenier = tokenizer\n",
    "        self.part = part\n",
    "        self.data = None\n",
    "        self._load_data()\n",
    "\n",
    "    def _load_data(self):\n",
    "        \"\"\"\n",
    "            加载数据\n",
    "        \"\"\"\n",
    "        if self.data:\n",
    "            print(\"数据集已经构建过了\")\n",
    "            return\n",
    "        elif os.path.exists(os.path.join(\".cache\", \"data.lxh\")):\n",
    "            print(\"从缓存中读取数据\")\n",
    "            # 原始数据\n",
    "            data = joblib.load(filename=os.path.join(\".cache\", \"data.lxh\"))\n",
    "            # 80% 训练集\n",
    "            # 20% 测试集\n",
    "            nums = int(len(data) * 0.80)\n",
    "            self.data = data[:nums] if self.part == \"train\" else data[nums:]\n",
    "            return\n",
    "        # 从零读取\n",
    "        data = pd.read_csv(filepath_or_buffer=self.data_file, sep=\"\\t\", header=None)\n",
    "        data = data.sample(frac=1).to_numpy()\n",
    "        # 保存数据\n",
    "        joblib.dump(value=data, filename=os.path.join(\".cache\", \"data.lxh\"))\n",
    "        # 数据截取\n",
    "        nums = int(len(data) * 0.80)\n",
    "        self.data = data[:nums] if self.part == \"train\" else data[nums:]\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "            通过索引来访问样本\n",
    "        \"\"\"\n",
    "        src, tgt = self.data[idx]\n",
    "        src = tokenizer.split_english_sentence(src)\n",
    "        tgt = tokenizer.split_chinese_sentence(tgt)\n",
    "        return src, len(src), tgt, len(tgt)\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "            返回该数据集的样本个数\n",
    "        \"\"\"\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1e8b153-9801-47a0-bafb-dbf42bcdb38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch, tokenizer=tokenizer):\n",
    "    \"\"\"\n",
    "        回调函数\n",
    "    \"\"\"\n",
    "    # 按 src_len 逆序\n",
    "    batch = sorted(batch, key=lambda ele: ele[1], reverse=True)\n",
    "    # 分拆成4个集合\n",
    "    src_sentences, src_lens, tgt_sentences, tgt_lens = zip(*batch)\n",
    "    # 1. src 转 id\n",
    "    src_max_len = src_lens[0]\n",
    "    src_idxes = []\n",
    "    for src_sentence in src_sentences:\n",
    "        src_idxes.append(tokenizer.encode_src(src_sentence, src_max_len))\n",
    "\n",
    "    # 2. tgt 转 id\n",
    "    tgt_max_len = max(tgt_lens)\n",
    "    tgt_idxes = []\n",
    "    for tgt_sentence in tgt_sentences:\n",
    "        tgt_idxes.append(tokenizer.encode_tgt(tgt_sentence, tgt_max_len))\n",
    "\n",
    "    # 所有数据转张量 torch.long\n",
    "    # [src_max_len, batch_size]\n",
    "    src_idxes = torch.tensor(data=src_idxes, dtype=torch.long).t()\n",
    "    # (batch_size, )\n",
    "    src_lens = torch.tensor(data=src_lens, dtype=torch.long)\n",
    "    # [tgt_max_len + 2, batch_size]\n",
    "    tgt_idxes = torch.tensor(data=tgt_idxes, dtype=torch.long).t()\n",
    "    # (batch_size, )\n",
    "    tgt_lens = torch.tensor(data=tgt_lens, dtype=torch.long)\n",
    "\n",
    "    return src_idxes, src_lens, tgt_idxes, tgt_lens\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a53f58c-7829-4c02-ad7a-26c94ae32e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "从缓存中读取数据\n",
      "从缓存中读取数据\n"
     ]
    }
   ],
   "source": [
    "# 训练集\n",
    "train_dataset = Seq2SeqDataset(data_file=\"data.txt\", part=\"train\")\n",
    "train_dataloader = DataLoader(dataset=train_dataset, \n",
    "                              shuffle=True, \n",
    "                              batch_size=32,\n",
    "                              collate_fn=collate_fn)\n",
    "# 测试集\n",
    "test_dataset = Seq2SeqDataset(data_file=\"data.txt\", part=\"test\")\n",
    "test_dataloader = DataLoader(dataset=test_dataset, \n",
    "                              shuffle=False, \n",
    "                              batch_size=32,\n",
    "                              collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "546046fd-100e-40af-8063-32b11e7f8708",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\63447\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.355 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([14, 32]) tensor([14, 13, 13, 10,  9,  9,  9,  8,  8,  8,  8,  8,  8,  8,  8,  8,  7,  7,\n",
      "         6,  6,  6,  6,  6,  6,  6,  6,  5,  5,  5,  5,  4,  3]) torch.Size([15, 32]) tensor([ 8,  8, 10,  6,  8,  8,  9,  6,  6,  5,  8,  8, 13,  8,  7, 11,  7,  6,\n",
      "         5,  6,  7,  7,  3,  6,  6,  8,  7,  6,  6,  6,  5,  2])\n"
     ]
    }
   ],
   "source": [
    "for src, src_lens, tgt, tgt_lens in test_dataloader:\n",
    "    print(src.shape, src_lens, tgt.shape, tgt_lens)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386b6164-934b-4910-8756-a8504dccdd4c",
   "metadata": {},
   "source": [
    "### 4. 编码器设计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6baba9f0-b8bd-4957-9021-550b0e2d5c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4163599-b557-4b6f-b670-1f8457e9140a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "        自定义一个编码器，处理 src\n",
    "            - `Seq` 2 Seq\n",
    "            - 只是 一个很单纯 的 RNN\n",
    "            - 没有任何的差别\n",
    "    \"\"\"\n",
    "    def __init__(self, num_embeddings=len(tokenizer.src_token2idx), embedding_dim=256):\n",
    "        # 仅用于上坟，没有任何其他作用！\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(num_embeddings=num_embeddings,\n",
    "                                 embedding_dim=embedding_dim, \n",
    "                                 padding_idx=tokenizer.src_token2idx.get(\"<PAD>\"))\n",
    "        self.gru = nn.GRU(input_size=embedding_dim, \n",
    "                          hidden_size=embedding_dim)\n",
    "\n",
    "    def forward(self, src, src_lens):\n",
    "        \"\"\"\n",
    "            前向传播\n",
    "                - 消除 PAD 影响\n",
    "        \"\"\"\n",
    "        # [src_max_len, batch_size] --> [src_max_len, batch_size, embed_dim]\n",
    "        src = self.embed(src)\n",
    "        # 压紧被填充的序列\n",
    "        src = nn.utils.rnn.pack_padded_sequence(input=src, lengths=src_lens, batch_first=False)\n",
    "        out, hn = self.gru(src)\n",
    "        return hn[0, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7fccba3-02bf-4d6c-9b61-d44da45166f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db95be48-8de6-4163-85d3-1a6b2b493342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 256])\n"
     ]
    }
   ],
   "source": [
    "for src, src_lens, tgt, tgt_lens in train_dataloader:\n",
    "    memory = encoder(src, src_lens)\n",
    "    print(memory.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577d494a-f1e2-4659-bf1a-31a6ca5a9196",
   "metadata": {},
   "source": [
    "### 5. 解码器设计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "437503e5-833b-4557-b02b-3a81af40efcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78c0f809-495e-4288-be4b-4507e061191a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "        实现解码器：\n",
    "            - 训练时：\n",
    "                - 考虑 teacher forcing\n",
    "            - 推理时：\n",
    "                - 考虑 自回归\n",
    "    \"\"\"\n",
    "    def __init__(self, num_embeddings=len(tokenizer.tgt_token2idx), embedding_dim=256):\n",
    "        super().__init__()\n",
    "        # 向量化的过程\n",
    "        self.embed = nn.Embedding(num_embeddings=num_embeddings, \n",
    "                                  embedding_dim=embedding_dim, \n",
    "                                  padding_idx=tokenizer.tgt_token2idx.get(\"<PAD>\"))\n",
    "        \n",
    "        # 手动挡，分步特征抽取，实现自回归逻辑！！！\n",
    "        self.gru_cell = nn.GRUCell(input_size=embedding_dim,\n",
    "                                  hidden_size=embedding_dim)\n",
    "        \n",
    "        # 输出 embed_dim --> dict_len\n",
    "        self.out = nn.Linear(in_features=embedding_dim, out_features=len(tokenizer.tgt_token2idx))\n",
    "    \n",
    "    def forward(self, context, tgt, tgt_lens):\n",
    "        \"\"\"\n",
    "            训练时的正向推理：\n",
    "\n",
    "                context: 上下文向量，中间表达\n",
    "                tgt：标签\n",
    "                tgt_lens：生成的句子的有效长度（不包含 <SOS>和<EOS>）     \n",
    "        \"\"\"\n",
    "        # 生成侧的输入\n",
    "        tgt_input = tgt[:-1, :]\n",
    "        # 生成侧的输出\n",
    "        tgt_output = tgt[1:, :]\n",
    "        # 输入序列长度和批量大小\n",
    "        SEQ_LEN, BATCH_SIZE = tgt_input.shape\n",
    "        # 准备初始状态\n",
    "        hn = context\n",
    "        # 有多少步，就循环多少次\n",
    "        outs = []\n",
    "        step_input = self.embed(tgt_input[0, :].view(1, -1))[0, :, :]\n",
    "        \n",
    "        for step in range(SEQ_LEN):\n",
    "            # 正向传播\n",
    "            hn = self.gru_cell(step_input, hn)\n",
    "            # 生成结果\n",
    "            y_pred = self.out(hn)\n",
    "            # 保留所有生成的结果（做交叉熵损失用）\n",
    "            outs.append(y_pred)\n",
    "            \n",
    "            # 训练时采用 50% 的概率去使用 teacher forcing 优化策略\n",
    "            teacher_forcing = random.random() > 0.5\n",
    "            if teacher_forcing:\n",
    "                step_input = self.embed(tgt_input[step + 1, :].view(1, -1))[0, :, :]\n",
    "            else:\n",
    "                y_pred = y_pred.argmax(dim=-1, keepdim=True).view(1, -1)\n",
    "                step_input = self.embed(y_pred)[0, :, :]\n",
    "        \n",
    "        return outs, tgt_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "715fe11c-17d7-4949-b83e-2d7d0b778f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder()\n",
    "decoder = Decoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "96d09b41-0338-48b5-a4ac-fab9afc99b7a",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 15 is out of bounds for dimension 0 with size 15",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m context \u001b[38;5;241m=\u001b[39m encoder(src, src_lens)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# 2. 实现解码过程\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m outs, tgt_lens \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_lens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mD:\\software\\Anaconda\\envs\\py39_normal\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\software\\Anaconda\\envs\\py39_normal\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[15], line 54\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[1;34m(self, context, tgt, tgt_lens)\u001b[0m\n\u001b[0;32m     52\u001b[0m teacher_forcing \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mrandom() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m teacher_forcing:\n\u001b[1;32m---> 54\u001b[0m     step_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed(\u001b[43mtgt_input\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))[\u001b[38;5;241m0\u001b[39m, :, :]\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     56\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m y_pred\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: index 15 is out of bounds for dimension 0 with size 15"
     ]
    }
   ],
   "source": [
    "for src, src_lens, tgt, tgt_lens in train_dataloader:\n",
    "    # 1. 实现编码过程\n",
    "    context = encoder(src, src_lens)\n",
    "    # 2. 实现解码过程\n",
    "    outs, tgt_lens = decoder(context, tgt, tgt_lens)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ccd81c-591e-43e6-91dd-500e531b004f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62fd85a-30f0-483e-b7a5-f94e6a60a1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b36a2d-ccd3-457b-915b-dd115f3d93d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff2bf12-31bc-4414-b340-5a8a5af195bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
