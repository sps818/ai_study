{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5464e70-f89b-456b-a87e-b1b7b866b0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# jieba分词器，os目录，opencc繁体简体互转\n",
    "import jieba, os, opencc, joblib\n",
    "# 进度条\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0d4add-9e2b-49be-af9b-b741ecc1ada7",
   "metadata": {},
   "source": [
    "### 1. 构建分词器,并在分词器内读取原始数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84f04322-105c-49da-bcf6-9c2ec16482a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer():\n",
    "    \"\"\"\n",
    "        自定义一个分词器\n",
    "            - 1. 根据输入的语料，构建字典\n",
    "            - 2. 输入src的句子，输出对应的id\n",
    "            - 3. 输入tgt的句子，输出对应的id\n",
    "            - 4. 输入tgt的id，输出tgt的句子\n",
    "    \"\"\"\n",
    "    def __init__(self, data_file):\n",
    "        \"\"\"\n",
    "            分词器初始化\n",
    "                - 默认：根据输入的语料，构建字典\n",
    "        \"\"\"\n",
    "        self.data_file = data_file\n",
    "        self.data = None\n",
    "        self.src_token2idx = None\n",
    "        self.idx2src_token = None\n",
    "        self.tgt_token2idx = None\n",
    "        self.idx2tgt_token = None\n",
    "        self._build_dict()\n",
    "\n",
    "    def _build_dict(self):\n",
    "        \"\"\"\n",
    "            构建字典\n",
    "        \"\"\"\n",
    "        # 判断是否已经构建过\n",
    "        if self.src_token2idx:\n",
    "            print(\"字典已经构建过了\")\n",
    "            return\n",
    "        elif os.path.exists(os.path.join(\".cache\", \"dicts.pmc\")):\n",
    "            print(\"从缓存中读取字典\")\n",
    "            self.src_token2idx, self.idx2src_token, self.tgt_token2idx, self.idx2tgt_token = joblib.load(filename=os.path.join(\".cache\", \"dicts.pmc\"))\n",
    "            return\n",
    "\n",
    "        # 读取数据\n",
    "        self.data = pd.read_csv(filepath_or_buffer=self.data_file, sep=\"\\t\", names=[\"src\", \"tgt\"])\n",
    "        \n",
    "        # 获取行数和列数\n",
    "        rows, cols = self.data.shape\n",
    "        # 构建词典\n",
    "        src_tokens = {\"<UNK>\", \"<PAD>\", \"<SOS>\", \"<EOS>\"}\n",
    "        tgt_tokens = {\"<UNK>\", \"<PAD>\", \"<SOS>\", \"<EOS>\"}\n",
    "        for row_idx in tqdm(range(rows)):\n",
    "            src, tgt = self.data.loc[row_idx, :]\n",
    "            src_tokens.update(set(self._split_english_sentence(src)))\n",
    "            tgt_tokens.update(set(self._split_chinese_sentence(tgt)))  \n",
    "\n",
    "        # 构建src的字典\n",
    "        self.src_token2idx = {token: idx for idx, token in enumerate(src_tokens)}\n",
    "        self.idx2src_token = {idx: token for token, idx in self.src_token2idx.items()}\n",
    "        # 构建tgt的字典\n",
    "        self.tgt_token2idx = {token: idx for idx, token in enumerate(tgt_tokens)}\n",
    "        self.idx2tgt_token = {idx: token for token, idx in self.tgt_token2idx.items()}\n",
    "\n",
    "        # 保存构建好的字典\n",
    "        dicts = [self.src_token2idx, self.idx2src_token, self.tgt_token2idx, self.idx2tgt_token]\n",
    "        joblib.dump(value=dicts, filename=os.path.join(\".cache\", \"dicts.pmc\"))\n",
    "    \n",
    "    def _split_english_sentence(self, sentence):\n",
    "        \"\"\"\n",
    "            英文句子切分\n",
    "        \"\"\"\n",
    "        sentence = sentence.strip()\n",
    "        # 分词\n",
    "        tokens = [word for word in jieba.lcut(sentence.lower()) if word not in (\"\", \" \", \"'\")]\n",
    "        return tokens\n",
    "\n",
    "    def _split_chinese_sentence(self, sentence):\n",
    "        \"\"\"\n",
    "            中文句子切分  \n",
    "        \"\"\"\n",
    "        t2s_converter = opencc.OpenCC(config='t2s')\n",
    "        sentence = sentence.strip()\n",
    "        # 分词\n",
    "        tokens = jieba.lcut(t2s_converter.convert(text=sentence))\n",
    "        return tokens\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "            返回必要的信息\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.src_token2idx:\n",
    "            out = f\"Tokenizer: [src: {len(self.src_token2idx)}, tgt: {len(self.tgt_token2idx)}]\"\n",
    "        else:\n",
    "            out = f\"尚未构建字典\"\n",
    "        return out\n",
    "        \n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "            返回必要的信息\n",
    "        \"\"\"\n",
    "        return self.__str__()\n",
    "\n",
    "    def encode_src(self, src_sentence, src_max_len):\n",
    "        \"\"\"\n",
    "            把分词后的句子 变成id\n",
    "        \"\"\"\n",
    "        src_idx = [self.src_token2idx.get(token, self.src_token2idx.get(\"<UNK>\")) for token in src_sentence]\n",
    "        src_idx = (src_idx + [self.src_token2idx.get(\"<PAD>\")] * src_max_len)[: src_max_len]\n",
    "        return src_idx\n",
    "\n",
    "    def encode_tgt(self, tgt_sentence, tgt_max_len):\n",
    "        \"\"\"\n",
    "            把分词后的target句子 变成id\n",
    "            句子都要以<SOS>开头以<EOS>结束。\n",
    "        \"\"\"\n",
    "        tgt_sentence = [\"<SOS>\"] + tgt_sentence + [\"<EOS>\"]\n",
    "        tgt_max_len += 2\n",
    "        tgt_idx = [self.tgt_token2idx.get(token, self.tgt_token2idx.get(\"<UNK>\")) for token in tgt_sentence]\n",
    "        tgt_idx = (tgt_idx + [self.tgt_token2idx.get(\"<PAD>\")] * tgt_max_len)[: tgt_max_len]\n",
    "        return tgt_idx        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748f0afd-587f-4426-9722-174d17fbbc92",
   "metadata": {},
   "source": [
    "### 3. 数据打包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "825d4afc-a851-49cb-aefc-a6c076c8b1c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "从缓存中读取字典\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(data_file=\"./data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a775c10e-b78a-4ca7-80d2-e26900bd31dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tokenizer: [src: 7106, tgt: 12547]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "994f6545-af36-42dc-89f0-c2603536de96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9980bbba-56bc-42db-abdf-1618d099cf64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2seqDataset(Dataset):\n",
    "    \"\"\"\n",
    "        自定义数据集\n",
    "    \"\"\"\n",
    "    def __init__(self, data_file, part=\"train\", tokenizer=tokenizer):\n",
    "        \"\"\"\n",
    "            初始化，接受超参\n",
    "        \"\"\"\n",
    "        self.data_file = data_file\n",
    "        self.part = part\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = None\n",
    "        self._load_data()\n",
    "\n",
    "    def _load_data(self):\n",
    "        \"\"\"\n",
    "            加载数据\n",
    "        \"\"\"\n",
    "        if self.data:\n",
    "            print(\"数据集已经构建过了\")\n",
    "            return\n",
    "        elif os.path.exists(os.path.join(\".cache\", \"data.pmc\")):\n",
    "            print(\"从缓存中读取数据\")\n",
    "            # 从缓存获取全部数据集\n",
    "            data = joblib.load(filename=os.path.join(\".cache\", \"data.pmc\"))\n",
    "            # 全部数据集中80%作为训练集20%作为验证集\n",
    "            nums = int(len(data) * 0.8)\n",
    "            self.data = data[:nums] if self.part == \"train\" else data[nums:]\n",
    "            return\n",
    "        \n",
    "        # 从零开始读取\n",
    "        data = pd.read_csv(filepath_or_buffer=self.data_file, sep=\"\\t\", names=[\"src\", \"tgt\"])\n",
    "        # 打乱顺序并,frac=1是100%采样的意思\n",
    "        data = data.sample(frac=1).to_numpy()\n",
    "\n",
    "        # 保存所有数据数据\n",
    "        joblib.dump(value=data, filename=os.path.join(\".cache\", \"data.pmc\"))\n",
    "\n",
    "        # 加载数据\n",
    "        nums = int(len(data) * 0.8)\n",
    "        self.data = data[:nums] if self.part == \"train\" else data[nums:]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "            通过索引访问样本\n",
    "        \"\"\"\n",
    "        src, tgt = self.data[idx]\n",
    "        src = tokenizer._split_english_sentence(sentence=src)\n",
    "        tgt = tokenizer._split_chinese_sentence(sentence=tgt)\n",
    "        return src, len(src), tgt, len(tgt)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "           返回该数据集的样本个数 \n",
    "        \"\"\"\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69c47b5f-0994-43da-882b-ea72e8303047",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch, tokenizer=tokenizer):\n",
    "    \"\"\"\n",
    "        回调函数\n",
    "    \"\"\"\n",
    "    # 按src_len 逆序排序\n",
    "    batch = sorted(batch, key=lambda ele: ele[1], reverse=True)\n",
    "    # 分拆成4个集合\n",
    "    src_sentences, src_lens, tgt_sentences, tgt_lens = zip(*batch)\n",
    "    # print(src_sentences)\n",
    "    # print(src_lens)\n",
    "    # print(tgt_sentences)\n",
    "    # print(tgt_lens)\n",
    "\n",
    "    # 1. src 转 id\n",
    "    src_max_len = src_lens[0]\n",
    "    src_idxes = []\n",
    "    for src_sentence in src_sentences:\n",
    "        src_idxes.append(tokenizer.encode_src(src_sentence, src_max_len))\n",
    "\n",
    "    # 2. tgt 转 id\n",
    "    tgt_max_len = max(tgt_lens)\n",
    "    tgt_idxes = []\n",
    "    for tgt_sentence in tgt_sentences:\n",
    "        tgt_idxes.append(tokenizer.encode_tgt(tgt_sentence, tgt_max_len))\n",
    "    # print(tgt_idxes)\n",
    "\n",
    "    # 所有数据转张量 torch.long\n",
    "    # [src_max_len, batch_size]\n",
    "    src_idxes = torch.tensor(data=src_idxes, dtype=torch.long).t()\n",
    "    # (batch_size, )\n",
    "    src_lens = torch.tensor(data=src_lens, dtype=torch.long)\n",
    "    # [tgt_max_len + 2, batch_size]\n",
    "    tgt_idxes = torch.tensor(data=tgt_idxes, dtype=torch.long).t()\n",
    "    # (batch_size, )\n",
    "    tgt_lens = torch.tensor(data=tgt_lens, dtype=torch.long)\n",
    "\n",
    "    return src_idxes, src_lens, tgt_idxes, tgt_lens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2826c0f-0e65-44ec-891d-2903448ba70a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "从缓存中读取数据\n",
      "从缓存中读取数据\n"
     ]
    }
   ],
   "source": [
    "# 训练集\n",
    "train_dataset = Seq2seqDataset(data_file=\"./data.txt\", part=\"train\")\n",
    "train_dataloader = DataLoader(dataset=train_dataset, \n",
    "                              shuffle=True, \n",
    "                              batch_size=32, \n",
    "                              collate_fn=collate_fn)\n",
    "\n",
    "# 测试集\n",
    "test_dataset = Seq2seqDataset(data_file=\"./data.txt\", part=\"test\")\n",
    "test_dataloader = DataLoader(dataset=test_dataset, \n",
    "                              shuffle=False, \n",
    "                              batch_size=32, \n",
    "                              collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f8b9b2d-b5ab-4afa-9847-12c455c7adc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\63447\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.410 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 32]) torch.Size([32]) torch.Size([15, 32]) torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "for src_idxes, src_lens, tgt_idxes, tgt_lens in train_dataloader:\n",
    "    print(src_idxes.shape, src_lens.shape, tgt_idxes.shape, tgt_lens.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9de51e14-7f9c-4d4b-a5dd-dedfac1ab868",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ship'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.idx2src_token.get(5991)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff073175-7a81-4d14-ac18-be51cbbe4cc7",
   "metadata": {},
   "source": [
    "### 4. 编码器设计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1af71722-2754-4ee7-ac2a-83fa5949cd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d68e6b90-ef17-4aca-9fba-ab238cc27d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "        自定义一个编码器，处理 src\n",
    "            - `Seq` 2 Seq\n",
    "            - 只是 一个很单纯 的 RNN\n",
    "            - 没有任何的差别\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_embeddings=len(tokenizer.src_token2idx), embedding_dim=256):\n",
    "        # 仅用于上坟，没有任何其他作用！\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(num_embeddings=num_embeddings,\n",
    "                                 embedding_dim=embedding_dim, \n",
    "                                 padding_idx=tokenizer.src_token2idx.get(\"<PAD>\"))\n",
    "        self.gru = nn.GRU(input_size=embedding_dim, \n",
    "                          hidden_size=embedding_dim)\n",
    "\n",
    "    def forward(self, src, src_lens):\n",
    "        \"\"\"\n",
    "            前向传播\n",
    "                - 消除 PAD 影响\n",
    "        \"\"\"\n",
    "        # [src_max_len, batch_size] --> [src_max_len, batch_size, embed_dim]\n",
    "        src = self.embed(src)\n",
    "        # 压紧被填充的序列\n",
    "        src = nn.utils.rnn.pack_padded_sequence(input=src, lengths=src_lens, batch_first=False)\n",
    "        out, hn = self.gru(src\n",
    "                          )\n",
    "        return hn[0, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f69d076b-f835-4f87-a313-867325691d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6132bba9-a983-41fd-b109-458661a949c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 256])\n"
     ]
    }
   ],
   "source": [
    "for src, src_lens, tgt, tgt_lens in train_dataloader:\n",
    "    memory = encoder(src, src_lens)\n",
    "    print(memory.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b086b37-23cc-497f-9b2b-6216129da535",
   "metadata": {},
   "source": [
    "### 5. 解码器设计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "40bfc786-2522-40c2-b3ec-d1b0204ce102",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2599c2c7-6ee7-4691-a3b3-7ffd18cb1b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "        实现解码器：\n",
    "            - 训练时：\n",
    "                - 考虑 teacher forcing\n",
    "            - 推理时：\n",
    "                - 考虑 自回归\n",
    "    \"\"\"\n",
    "    def __init__(self, num_embeddings=len(tokenizer.tgt_token2idx), embedding_dim=256):\n",
    "        super().__init__()\n",
    "        # 向量化的过程\n",
    "        self.embed = nn.Embedding(num_embeddings=num_embeddings, \n",
    "                                  embedding_dim=embedding_dim, \n",
    "                                  padding_idx=tokenizer.tgt_token2idx.get(\"<PAD>\"))\n",
    "        \n",
    "        # 手动挡，分步特征抽取，实现自回归逻辑！！！\n",
    "        self.gru_cell = nn.GRUCell(input_size=embedding_dim,\n",
    "                                  hidden_size=embedding_dim)\n",
    "        \n",
    "        # 输出 embed_dim --> dict_len\n",
    "        self.out = nn.Linear(in_features=embedding_dim, out_features=len(tokenizer.tgt_token2idx))\n",
    "    \n",
    "    def forward(self, context, tgt, tgt_lens):\n",
    "        \"\"\"\n",
    "            训练时的正向推理：\n",
    "\n",
    "                context: 上下文向量，中间表达\n",
    "                tgt：标签\n",
    "                tgt_lens：生成的句子的有效长度（不包含 <SOS>和<EOS>）     \n",
    "        \"\"\"\n",
    "        # 生成侧的输入\n",
    "        tgt_input = tgt[:-1, :]\n",
    "        # 生成侧的输出\n",
    "        tgt_output = tgt[1:, :]\n",
    "        # 输入序列长度和批量大小\n",
    "        SEQ_LEN, BATCH_SIZE = tgt_input.shape\n",
    "        # 准备初始状态\n",
    "        hn = context\n",
    "        # 有多少步，就循环多少次\n",
    "        outs = []\n",
    "        step_input = self.embed(tgt_input[0, :].view(1, -1))[0, :, :]\n",
    "        \n",
    "        for step in range(SEQ_LEN):\n",
    "            # 正向传播\n",
    "            hn = self.gru_cell(step_input, hn)\n",
    "            # 生成结果\n",
    "            y_pred = self.out(hn)\n",
    "            # 保留所有生成的结果（做交叉熵损失用）\n",
    "            outs.append(y_pred)\n",
    "            \n",
    "            # 训练时采用 50% 的概率去使用 teacher forcing 优化策略\n",
    "            teacher_forcing = random.random() > 0.5\n",
    "            if teacher_forcing:\n",
    "                step_input = self.embed(tgt_input[step + 1, :].view(1, -1))[0, :, :]\n",
    "            else:\n",
    "                y_pred = y_pred.argmax(dim=-1, keepdim=True).view(1, -1)\n",
    "                step_input = self.embed(y_pred)[0, :, :]\n",
    "        \n",
    "        return outs, tgt_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a3f33c05-f058-44ed-8e0b-9be32b3edec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder()\n",
    "decoder = Decoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "75644d10-e8d5-4ab4-b8e7-3ab42ef1a250",
   "metadata": {},
   "outputs": [],
   "source": [
    "for src, src_lens, tgt, tgt_lens in train_dataloader:\n",
    "    # 1. 实现编码过程\n",
    "    context = encoder(src, src_lens)\n",
    "    # 2. 实现解码过程\n",
    "    outs, tgt_lens = decoder(context, tgt, tgt_lens)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a41ee94-fbbe-4ee8-bc58-c75bdbb2a213",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c9ad8655-2652-437b-a498-8c4feb4d21e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([8, 9, 8, 8, 7, 7, 8, 6, 6, 7, 8, 6, 5, 7, 6, 7, 8, 5, 6, 7, 4, 7, 6, 5,\n",
       "        6, 7, 4, 6, 5, 5, 2, 4])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bf6f70-2975-420a-8499-904d44327db8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
