{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a519495-7b12-4c81-97ec-d5888b6121ae",
   "metadata": {},
   "source": [
    "### 1. 读取原始数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a828c837-1cc7-49c1-b5cd-5e38ce6e99ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "import joblib\n",
    "import jieba\n",
    "import opencc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18c7164-9af5-4137-9a0a-57983875d979",
   "metadata": {},
   "source": [
    "### 2. 构建分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f04d8037-12c5-4a5e-a620-bf1a0fd8c472",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer(object):\n",
    "    \"\"\"\n",
    "        自定义一个分词器，实现基本功能：\n",
    "            - 1. 根据输入的语料，构建字典\n",
    "            - 2. 输入src的句子，输出对应的id\n",
    "            - 3. 输入tgt的句子，输出对应的id\n",
    "            - 4. 输入tgt的id，输出tgt的句子\n",
    "    \"\"\"\n",
    "    def __init__(self, data_file):\n",
    "        \"\"\"\n",
    "            分词器初始化\n",
    "                - 默认：根据输入的语料，构建字典\n",
    "        \"\"\"\n",
    "        self.data_file = data_file\n",
    "        # 输入侧 src --> source\n",
    "        self.src_token2idx = None\n",
    "        self.src_idx2token = None\n",
    "        # 输出侧 tgt --> target\n",
    "        self.tgt_token2idx = None\n",
    "        self.tgt_idx2token = None\n",
    "        # 构建字典\n",
    "        self._build_dict()\n",
    "    \n",
    "    def _build_dict(self):\n",
    "        \"\"\"\n",
    "            构建字典\n",
    "        \"\"\"\n",
    "        if self.src_token2idx:\n",
    "            print(\"字典已经构建过了\")\n",
    "            return\n",
    "        elif os.path.exists(os.path.join(\".cache\", \"dicts.lxh\")):\n",
    "            print(\"从缓存中读取字典\")\n",
    "            self.src_token2idx, self.src_idx2token, self.tgt_token2idx, self.tgt_idx2token = joblib.load(filename=os.path.join(\".cache\", \"dicts.lxh\"))\n",
    "            return\n",
    "        \n",
    "        # 从零构建字典\n",
    "        data = pd.read_csv(filepath_or_buffer=self.data_file, sep=\"\\t\", header=None)\n",
    "        data.columns = [\"src\", \"tgt\"]\n",
    "        rows, cols  = data.shape\n",
    "        # 构建词典\n",
    "        src_tokens = {\"<UNK>\", \"<PAD>\", \"<SOS>\", \"EOS\"}\n",
    "        tgt_tokens = {\"<UNK>\", \"<PAD>\", \"<SOS>\", \"EOS\"}\n",
    "        for row_idx in tqdm(range(rows)):\n",
    "            src, tgt = data.loc[row_idx, :]\n",
    "            src_tokens.update(set(self.split_english_sentence(src)))\n",
    "            tgt_tokens.update(set(self.split_chinese_sentence(tgt)))\n",
    "        \n",
    "        # 构建 src 的 字典\n",
    "        self.src_token2idx = {token: idx for idx, token in enumerate(src_tokens)}\n",
    "        self.src_idx2token = {idx: token for token, idx in self.src_token2idx.items()}\n",
    "\n",
    "        # 构建 tgt 的 字典\n",
    "        self.tgt_token2idx = {token: idx for idx, token in enumerate(tgt_tokens)}\n",
    "        self.tgt_idx2token = {idx: token for token, idx in self.tgt_token2idx.items()}\n",
    "\n",
    "        # 保存\n",
    "        dicts = [self.src_token2idx, self.src_idx2token, self.tgt_token2idx, self.tgt_idx2token]\n",
    "        joblib.dump(value=dicts, filename=os.path.join(\".cache\", \"dicts.lxh\"))\n",
    "        \n",
    "    def split_english_sentence(self, sentecne):\n",
    "        \"\"\"\n",
    "            英文句子切分\n",
    "        \"\"\"\n",
    "        sentecne = sentecne.strip()\n",
    "        # 小写\n",
    "        tokens = [token for token in jieba.lcut(sentecne.lower()) if token not in (\"\", \" \", \"'\")]\n",
    "        return tokens\n",
    "    \n",
    "    def split_chinese_sentence(self, sentence):\n",
    "        \"\"\"\n",
    "            中文句子切分\n",
    "        \"\"\"\n",
    "        # 实例化一个繁体转简体的工具\n",
    "        converter = opencc.OpenCC(config=\"t2s\")\n",
    "        sentence = converter.convert(text=sentence)\n",
    "        # 分词\n",
    "        tokens = [token for token in jieba.lcut(sentence) if token not in [\"\", \" \"]]\n",
    "        return tokens\n",
    "        \n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "            返回必要的打印信息\n",
    "        \"\"\"\n",
    "        if self.src_token2idx:\n",
    "            out = f\"Tokenizer: [src: {len(self.src_token2idx)}, tgt: {len(self.tgt_token2idx)}]\"\n",
    "        else:\n",
    "            out = f\"尚无字典信息\"\n",
    "        return out\n",
    "    \n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "            返回必要的打印信息\n",
    "        \"\"\"\n",
    "        return self.__str__()\n",
    "\n",
    "    def encode_src(self, src_sentence, src_max_len):\n",
    "        \"\"\"\n",
    "            把分词后的句子，变成 id\n",
    "                - 按本批次的最大长度来填充\n",
    "                - src 不加 special token\n",
    "                    - EOS\n",
    "                    - SOS\n",
    "        \"\"\"\n",
    "        # 转换\n",
    "        src_idx = [self.src_token2idx.get(token, self.src_token2idx.get(\"<UNK>\")) for token in src_sentence]\n",
    "        # 填充\n",
    "        src_idx = (src_idx + [self.src_token2idx.get(\"<PAD>\")] * src_max_len)[:src_max_len]\n",
    "        return src_idx\n",
    "\n",
    "    def encode_tgt(self, tgt_sentence, tgt_max_len):\n",
    "        \"\"\"\n",
    "            把分词后的tgt句子变成 id\n",
    "                - <SOS>, 我, 爱, 北京, 天安门, ！, <EOS>, <PAD>, <PAD>\n",
    "        \"\"\"\n",
    "        # 前后加上 special token\n",
    "        tgt_sentence = [\"<SOS>\"] + tgt_sentence + [\"<EOS>\"]\n",
    "        tgt_max_len += 2\n",
    "        tgt_idx = [self.tgt_token2idx.get(token, self.tgt_token2idx.get(\"<UNK>\")) for token in tgt_sentence]\n",
    "        tgt_idx = (tgt_idx + [self.tgt_token2idx.get(\"<PAD>\")] * tgt_max_len)[:tgt_max_len]\n",
    "        return tgt_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64092dda-96ab-4f86-aabb-945f2bad26cf",
   "metadata": {},
   "source": [
    "### 3. 数据打包\n",
    "- 既要又要\n",
    "    - 既要批量化训练\n",
    "    - 又要消除填充PAD的噪声污染\n",
    "- collate_fn\n",
    "    - 手动排序！！！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e34ac30c-7758-46b0-b479-633dbb5e246b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import pandas\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b8358a9-f212-4362-8213-26a3b12110b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "从缓存中读取字典\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(data_file=\"data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b501b407-b66d-4ad3-a2cc-48f86120a9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqDataset(Dataset):\n",
    "    \"\"\"\n",
    "        自定义数据集\n",
    "    \"\"\"\n",
    "    def __init__(self, data_file, part=\"train\", tokenizer=tokenizer):\n",
    "        \"\"\"\n",
    "            初始化\n",
    "        \"\"\"\n",
    "        self.data_file = data_file\n",
    "        self.tokenier = tokenizer\n",
    "        self.part = part\n",
    "        self.data = None\n",
    "        self._load_data()\n",
    "\n",
    "    def _load_data(self):\n",
    "        \"\"\"\n",
    "            加载数据\n",
    "        \"\"\"\n",
    "        if self.data:\n",
    "            print(\"数据集已经构建过了\")\n",
    "            return\n",
    "        elif os.path.exists(os.path.join(\".cache\", \"data.lxh\")):\n",
    "            print(\"从缓存中读取数据\")\n",
    "            # 原始数据\n",
    "            data = joblib.load(filename=os.path.join(\".cache\", \"data.lxh\"))\n",
    "            # 80% 训练集\n",
    "            # 20% 测试集\n",
    "            nums = int(len(data) * 0.80)\n",
    "            self.data = data[:nums] if self.part == \"train\" else data[nums:]\n",
    "            return\n",
    "        # 从零读取\n",
    "        data = pd.read_csv(filepath_or_buffer=self.data_file, sep=\"\\t\", header=None)\n",
    "        # shuffle\n",
    "        data = data.sample(frac=1).to_numpy()\n",
    "        # 保存数据\n",
    "        joblib.dump(value=data, filename=os.path.join(\".cache\", \"data.lxh\"))\n",
    "        # 数据截取\n",
    "        nums = int(len(data) * 0.80)\n",
    "        self.data = data[:nums] if self.part == \"train\" else data[nums:]\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "            通过索引来访问样本\n",
    "        \"\"\"\n",
    "        src, tgt = self.data[idx]\n",
    "        src = tokenizer.split_english_sentence(src)\n",
    "        tgt = tokenizer.split_chinese_sentence(tgt)\n",
    "        return src, len(src), tgt, len(tgt)\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "            返回该数据集的样本个数\n",
    "        \"\"\"\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1e8b153-9801-47a0-bafb-dbf42bcdb38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch, tokenizer=tokenizer):\n",
    "    \"\"\"\n",
    "        回调函数\n",
    "            - src 样本，按照长度逆序排列\n",
    "    \"\"\"\n",
    "    # 按 src_len 逆序\n",
    "    batch = sorted(batch, key=lambda ele: ele[1], reverse=True)\n",
    "    # 分拆成4个集合\n",
    "    src_sentences, src_lens, tgt_sentences, tgt_lens = zip(*batch)\n",
    "    # 1. src 转 id\n",
    "    src_max_len = src_lens[0]\n",
    "    src_idxes = []\n",
    "    for src_sentence in src_sentences:\n",
    "        src_idxes.append(tokenizer.encode_src(src_sentence, src_max_len))\n",
    "\n",
    "    # 2. tgt 转 id\n",
    "    tgt_max_len = max(tgt_lens)\n",
    "    tgt_idxes = []\n",
    "    for tgt_sentence in tgt_sentences:\n",
    "        tgt_idxes.append(tokenizer.encode_tgt(tgt_sentence, tgt_max_len))\n",
    "\n",
    "    # 所有数据转张量 torch.long\n",
    "    # [src_max_len, batch_size]\n",
    "    src_idxes = torch.tensor(data=src_idxes, dtype=torch.long).t()\n",
    "    # (batch_size, )\n",
    "    src_lens = torch.tensor(data=src_lens, dtype=torch.long)\n",
    "    # [tgt_max_len + 2, batch_size]\n",
    "    tgt_idxes = torch.tensor(data=tgt_idxes, dtype=torch.long).t()\n",
    "    # (batch_size, )\n",
    "    tgt_lens = torch.tensor(data=tgt_lens, dtype=torch.long)\n",
    "\n",
    "    return src_idxes, src_lens, tgt_idxes, tgt_lens\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a53f58c-7829-4c02-ad7a-26c94ae32e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "从缓存中读取数据\n",
      "从缓存中读取数据\n"
     ]
    }
   ],
   "source": [
    "# 训练集\n",
    "train_dataset = Seq2SeqDataset(data_file=\"data.txt\", part=\"train\")\n",
    "train_dataloader = DataLoader(dataset=train_dataset, \n",
    "                              shuffle=True, \n",
    "                              batch_size=32,\n",
    "                              collate_fn=collate_fn)\n",
    "# 测试集\n",
    "test_dataset = Seq2SeqDataset(data_file=\"data.txt\", part=\"test\")\n",
    "test_dataloader = DataLoader(dataset=test_dataset, \n",
    "                              shuffle=False, \n",
    "                              batch_size=32,\n",
    "                              collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3af8b0c0-ea74-4158-8e0d-7d270349404e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.tgt_idx2token.get(3621)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "546046fd-100e-40af-8063-32b11e7f8708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for src, src_lens, tgt, tgt_lens in test_dataloader:\n",
    "#     print(src.shape, src_lens.shape, tgt, tgt_lens.shape)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386b6164-934b-4910-8756-a8504dccdd4c",
   "metadata": {},
   "source": [
    "### 4. 编码器设计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6baba9f0-b8bd-4957-9021-550b0e2d5c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4163599-b557-4b6f-b670-1f8457e9140a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "        自定义一个编码器，处理 src\n",
    "            - `Seq` 2 Seq\n",
    "            - 只是 一个很单纯 的 RNN\n",
    "            - 没有任何的差别\n",
    "    \"\"\"\n",
    "    def __init__(self, num_embeddings=len(tokenizer.src_token2idx), embedding_dim=256):\n",
    "        # 仅用于上坟，没有任何其他作用！\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(num_embeddings=num_embeddings,\n",
    "                                 embedding_dim=embedding_dim, \n",
    "                                 padding_idx=tokenizer.src_token2idx.get(\"<PAD>\"))\n",
    "        self.gru = nn.GRU(input_size=embedding_dim, \n",
    "                          hidden_size=embedding_dim)\n",
    "\n",
    "    def forward(self, src, src_lens):\n",
    "        \"\"\"\n",
    "            前向传播\n",
    "                - 消除 PAD 影响\n",
    "        \"\"\"\n",
    "        # [src_max_len, batch_size] --> [src_max_len, batch_size, embed_dim]\n",
    "        src = self.embed(src)\n",
    "        # 压紧被填充的序列\n",
    "        src = nn.utils.rnn.pack_padded_sequence(input=src, lengths=src_lens, batch_first=False)\n",
    "        out, hn = self.gru(src)\n",
    "        return hn[0, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7fccba3-02bf-4d6c-9b61-d44da45166f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db95be48-8de6-4163-85d3-1a6b2b493342",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\63447\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.428 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15, 32])\n",
      "torch.Size([32, 256])\n"
     ]
    }
   ],
   "source": [
    "for src, src_lens, tgt, tgt_lens in train_dataloader:\n",
    "    print(src.shape)\n",
    "    context = encoder(src, src_lens)\n",
    "    print(context.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577d494a-f1e2-4659-bf1a-31a6ca5a9196",
   "metadata": {},
   "source": [
    "### 5. 解码器设计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "437503e5-833b-4557-b02b-3a81af40efcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78c0f809-495e-4288-be4b-4507e061191a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "        实现解码器：\n",
    "            - 训练时：\n",
    "                - 考虑 teacher forcing\n",
    "            - 推理时：\n",
    "                - 考虑 自回归\n",
    "    \"\"\"\n",
    "    def __init__(self, num_embeddings=len(tokenizer.tgt_token2idx), embedding_dim=256):\n",
    "        super().__init__()\n",
    "        # 向量化的过程\n",
    "        self.embed = nn.Embedding(num_embeddings=num_embeddings, \n",
    "                                  embedding_dim=embedding_dim, \n",
    "                                  padding_idx=tokenizer.tgt_token2idx.get(\"<PAD>\"))\n",
    "        \n",
    "        # 手动挡，分步特征抽取，实现自回归逻辑！！！\n",
    "        self.gru_cell = nn.GRUCell(input_size=embedding_dim,\n",
    "                                  hidden_size=embedding_dim)\n",
    "        \n",
    "        # 输出 embed_dim --> dict_len\n",
    "        self.out = nn.Linear(in_features=embedding_dim, out_features=len(tokenizer.tgt_token2idx))\n",
    "    \n",
    "    def forward(self, context, tgt):\n",
    "        \"\"\"\n",
    "            训练时的正向推理：\n",
    "\n",
    "                context: 上下文向量，中间表达\n",
    "                tgt：标签\n",
    "                tgt_lens：生成的句子的有效长度（不包含 <SOS>和<EOS>）     \n",
    "        \"\"\"\n",
    "        # 生成侧的输入 \n",
    "        tgt_input = tgt[:-1, :]\n",
    "        # 生成侧的输出\n",
    "        tgt_output = tgt[1:, :]\n",
    "        # 实际步长，批量大小\n",
    "        SEQ_LEN, BATCH_SIZE = tgt_input.shape\n",
    "        # 准备初始状态\n",
    "        hn = context\n",
    "        # 有多少步，就循环多少次\n",
    "        outs = []\n",
    "        # [SOS]\n",
    "        # batch_size, embed_dim\n",
    "        step_input = self.embed(tgt_input[0, :].view(1, -1))[0, :, :]\n",
    "        \n",
    "        for step in range(SEQ_LEN):\n",
    "            # print(f\"第 {step} 步，总共 {SEQ_LEN} 步\")\n",
    "            # 正向传播\n",
    "            hn = self.gru_cell(step_input, hn)\n",
    "            # 生成结果\n",
    "            y_pred = self.out(hn)\n",
    "            # 保留所有生成的结果（做交叉熵损失用）\n",
    "            outs.append(y_pred)\n",
    "            # 为下一轮做准备\n",
    "            if step < SEQ_LEN - 1:\n",
    "                # 训练时采用 50% 的概率去使用 teacher forcing 优化策略\n",
    "                teacher_forcing = random.random() >= 0.5\n",
    "                if teacher_forcing:\n",
    "                    # 如果采用 teacher_forcing，则需要输入标准答案\n",
    "                    step_input = self.embed(tgt_input[step + 1, :].view(1, -1))[0, :, :]\n",
    "                else:\n",
    "                    # 如果不采用 teacher_forcing, 则 输入上一次生成的结果\n",
    "                    y_pred = y_pred.argmax(dim=-1, keepdim=True).view(1, -1)\n",
    "                    step_input = self.embed(y_pred)[0, :, :]\n",
    "        \n",
    "        return torch.stack(tensors=outs, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c28eb50d-a2eb-4131-87cb-055f478694ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    \"\"\"\n",
    "        定义一个完整模型\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "        \n",
    "    def forward(self, src, src_lens, tgt):\n",
    "        \"\"\"\n",
    "            训练时使用的前向传播\n",
    "        \"\"\"\n",
    "        context = self.encoder(src, src_lens)\n",
    "        outs = self.decoder(context, tgt)\n",
    "        return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bafcaa33-08cb-4406-8ca2-699be41794fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = Seq2Seq().to(device=device)\n",
    "epochs = 3\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "96d09b41-0338-48b5-a4ac-fab9afc99b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    for epoch in range(epochs):\n",
    "        running_epoch_losses = []\n",
    "        for src, src_lens, tgt, tgt_lens in train_dataloader:\n",
    "            # 数据搬家\n",
    "            src = src.to(device=device)\n",
    "            src_lens = src_lens\n",
    "            tgt = tgt.to(device=device)\n",
    "            tgt_lens = tgt_lens\n",
    "            # 1. 正向传播\n",
    "            outs = model(src, src_lens, tgt)\n",
    "            # 2， 损失计算\n",
    "            tgt = tgt[1:, :].contiguous().view(-1)\n",
    "            outs = outs.contiguous().view(-1, outs.size(-1))\n",
    "            loss = loss_fn(outs, tgt)\n",
    "            # 3. 反向传播\n",
    "            loss.backward()\n",
    "            # 4. 优化一步\n",
    "            optimizer.step()\n",
    "            # 5. 清空梯度\n",
    "            optimizer.zero_grad()\n",
    "            # 6. 累积损失\n",
    "            running_epoch_losses.append(loss.item())\n",
    "        \n",
    "        # 每隔一轮一下损失\n",
    "        running_epoch_loss = sum(running_epoch_losses) / len(running_epoch_losses)\n",
    "        print(f\"Epoch: {epoch} , train_loss: {running_epoch_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f5aeb3e5-98c5-4b34-810f-08b44f009b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 , train_loss: 2.8762763848324107\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[20], line 19\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# 4. 优化一步\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# 5. 清空梯度\u001b[39;00m\n\u001b[0;32m     21\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mD:\\software\\Anaconda\\envs\\py39_normal\\lib\\site-packages\\torch\\optim\\optimizer.py:487\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    482\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    483\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    484\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    485\u001b[0m             )\n\u001b[1;32m--> 487\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    490\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mD:\\software\\Anaconda\\envs\\py39_normal\\lib\\site-packages\\torch\\optim\\optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32mD:\\software\\Anaconda\\envs\\py39_normal\\lib\\site-packages\\torch\\optim\\adam.py:223\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    211\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    213\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    214\u001b[0m         group,\n\u001b[0;32m    215\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    220\u001b[0m         state_steps,\n\u001b[0;32m    221\u001b[0m     )\n\u001b[1;32m--> 223\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mD:\\software\\Anaconda\\envs\\py39_normal\\lib\\site-packages\\torch\\optim\\optimizer.py:154\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\software\\Anaconda\\envs\\py39_normal\\lib\\site-packages\\torch\\optim\\adam.py:784\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    781\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    782\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 784\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    803\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\software\\Anaconda\\envs\\py39_normal\\lib\\site-packages\\torch\\optim\\adam.py:430\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    428\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m    429\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 430\u001b[0m         denom \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbias_correction2_sqrt\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_\u001b[49m\u001b[43m(\u001b[49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    432\u001b[0m     param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n\u001b[0;32m    434\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570c6e20-2d19-4466-b8a0-2d21c333d72a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
