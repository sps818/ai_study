{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76b0e419-db7f-4081-8fd3-ceac87fad23c",
   "metadata": {},
   "source": [
    "### 1. Prelims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ce785cd-3dce-4ca8-881c-f78a8d4d8f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.functional import log_softmax, pad\n",
    "import math\n",
    "import time\n",
    "import copy\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30c4117b-3f15-45c0-9e38-693da39884cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 忽略警告信息\n",
    "# warnings.filterwarnings(action=\"ignore\")\n",
    "RUN_EXAMPLES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c825b936-3676-4a81-bbf4-b3aa1b1cb2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_interactive_notebook():\n",
    "    return __name__ == \"__main__\"\n",
    "\n",
    "\n",
    "def show_example(fn, args=[]):\n",
    "    if __name__ == \"__main__\" and RUN_EXAMPLES:\n",
    "        return fn(*args)\n",
    "\n",
    "\n",
    "def execute_example(fn, args=[]):\n",
    "    if __name__ == \"__main__\" and RUN_EXAMPLES:\n",
    "        fn(*args)\n",
    "\n",
    "\n",
    "class DummyOptimizer(torch.optim.Optimizer):\n",
    "    def __init__(self):\n",
    "        self.param_groups = [{\"lr\": 0}]\n",
    "        None\n",
    "\n",
    "    def step(self):\n",
    "        None\n",
    "\n",
    "    def zero_grad(self, set_to_none=False):\n",
    "        None\n",
    "\n",
    "\n",
    "class DummyScheduler:\n",
    "    def step(self):\n",
    "        None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7480ab-5a12-4d80-886e-50d3a6093c12",
   "metadata": {},
   "source": [
    "### 2. Part 1: Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e3f18fd-a279-4eef-85c7-d0103b9e1fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "         定义一个公共的 Encoder-Decoder 架构的框架\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
    "        \"\"\"\n",
    "            初始化方法：\n",
    "                - encoder：编码器（对象）\n",
    "                - decoder: 解码器（对象）\n",
    "                - src_embed：输入预处理（对象）\n",
    "                - tgt_embed：输出侧的输入预处理（对象）\n",
    "                - generator：生成处理（对象）\n",
    "        \"\"\"\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.generator = generator\n",
    "\n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        \"\"\"\n",
    "            输入并处理带掩码的输入和输出序列\n",
    "        \"\"\"\n",
    "        # 1，通过 encoder 获取中间表达\n",
    "        memory = self.encode(src, src_mask)\n",
    "        # 2，通过 decoder 获取最终结果\n",
    "        result = self.decode(memory, src_mask, tgt, tgt_mask)\n",
    "        return result\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        \"\"\"\n",
    "            编码器处理过程\n",
    "        \"\"\"\n",
    "        # 1，把输入的 id序列 变为向量并且加入位置编码\n",
    "        src_embed_pos = self.src_embed(src)\n",
    "        # 2，通过 encoder 获取中间表达\n",
    "        memory = self.encoder(src_embed_pos, src_mask)\n",
    "        return memory\n",
    "\n",
    "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
    "        \"\"\"\n",
    "            解码过程\n",
    "        \"\"\"\n",
    "        # 1，把 已经生成了的上文 id序列 变为 向量，再加上位置编码\n",
    "        tgt_embed_pos  = self.tgt_embed(tgt)\n",
    "        # 2，通过 decoder 进行加码\n",
    "        result = self.decoder(tgt_embed_pos, memory, src_mask, tgt_mask)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e3d3636-4c35-4dd3-9c6e-98298f3af8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"\"\"\n",
    "        把向量维度转换为词表长度，输出每个词的概率\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, dict_len):\n",
    "        \"\"\"\n",
    "            初始化\n",
    "                d_model：模型的向量维度，比如：512\n",
    "                dict_len：词表长度，比如：2万\n",
    "        \"\"\"\n",
    "        super(Generator, self).__init__()\n",
    "        self.proj = nn.Linear(in_features=d_model, out_features=dict_len)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "            前向映射过程\n",
    "        \"\"\"\n",
    "        # 1，特征映射（最后一个维度看作是特征维度）\n",
    "        x = self.proj(x)\n",
    "        return log_softmax(x, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794c72d3-4bba-472b-9c8d-7963666114c3",
   "metadata": {},
   "source": [
    "### 3. Encoder and Decoder Stacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0e7a7bb-998c-45fe-b4e4-3c59069d87b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clones(module, N):\n",
    "    \"\"\"\n",
    "        定义一个层的复制函数\n",
    "            - nn.ModuleList\n",
    "    \"\"\"\n",
    "    \n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23042709-4505-43f0-94b7-58ab77d20047",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "        第一个 Encoder，其由N个encoder layer 构成~    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, layer, N):\n",
    "        super(Encoder, self).__init__()\n",
    "        # 克隆 N 个层\n",
    "        self.layers = clones(layer, N)\n",
    "        # 定义一个 norm 层\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"\n",
    "            传入x及其mask，通过N层encoder layer 处理\n",
    "            mask: pad_mask 消除 pad 的影响\n",
    "        \"\"\"\n",
    "        # 经历 N 层处理\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        # 返回前，做一次 Norm 处理\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "231fa85c-edba-4f28-a4b2-a56011a4f448",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"\"\"\n",
    "        自定义 LayerNorm 层\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        \"\"\"\n",
    "            序列维度上做的\n",
    "            features: 特征的维度或个数\n",
    "            eps: epsilon 防止 标准差为零\n",
    "        \"\"\"\n",
    "        super(LayerNorm, self).__init__()\n",
    "        # 使用全1来初始化 类似于 weight\n",
    "        # nn.Parameter 定义可学习的参数\n",
    "        self.w = nn.Parameter(torch.ones(features))\n",
    "        # 使用全0来初始化 类似与 bias\n",
    "        self.b = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "            处理过程：\n",
    "                - 1，减去均值\n",
    "                - 2，除以标准差\n",
    "                - 3，可以在一定程度上还原\n",
    "        \"\"\"\n",
    "        # 1, 计算均值\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        # 2, 计算标准差\n",
    "        std = x.std(dim=-1, keepdim=True)\n",
    "        return self.w * (x - mean) / (std + self.eps) + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "acc60bfa-8904-4309-a0d1-c883e199984d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "        短接结构定义\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        # 定义一个 norm 层\n",
    "        self.norm = LayerNorm(size)\n",
    "        # 定义一个 dropout 层\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"执行过程\"\n",
    "        return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "acff7486-f6d5-4495-8ec9-7892f012c4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "        定义一个Encoder Layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer_conns = clones(SublayerConnection(size, dropout), 2)\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"\n",
    "            encoder layer 的执行过程\n",
    "        \"\"\"\n",
    "        # 1，先计算多头注意力\n",
    "        x = self.sublayer_conns[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
    "        # 2，再做 前馈处理\n",
    "        return self.sublayer_conns[1](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c06f330e-44ed-47cc-9c18-36e0431ea7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "        实现一个 Decoder\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, layer, N):\n",
    "        super(Decoder, self).__init__()\n",
    "        # 克隆 N 层\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "\n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        \"\"\"\n",
    "            解码过程\n",
    "                x: 已经生成了的上文\n",
    "                memory：中间表达\n",
    "                src_mask：pad_mask  标明 memory 中哪些是有效的\n",
    "                tgt_mask：pad_mask + subsequent_mask\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory, src_mask, tgt_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2635722d-f3a6-41a0-8af5-5e6f4024e7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "        定义一个解码器层\n",
    "            - self_attn\n",
    "            - src_attn\n",
    "            - feed_forward\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.size = size\n",
    "        self.self_attn = self_attn\n",
    "        self.cross_attn = src_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer_conns = clones(SublayerConnection(size, dropout), 3)\n",
    "\n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        \"\"\"\n",
    "            执行解码的过程\n",
    "        \"\"\"\n",
    "        # 1，对已经生成的前文内容进行自回归式提取特征\n",
    "        x = self.sublayer_conns[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
    "        # 2，参考问题的中间表达，进一步生成答案\n",
    "        x = self.sublayer_conns[1](x, lambda x: self.cross_attn(x, memory, memory, src_mask))\n",
    "        # 3，最后执行 前馈处理\n",
    "        return self.sublayer_conns[2](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "886d8065-65f7-46e7-9f38-744ae8354a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsequent_mask(size):\n",
    "    \"\"\"\n",
    "        未来词掩盖\n",
    "            - 只能看左边，不能看右边\n",
    "            - 我 | 爱 | 北京 | 天安门 | ！\n",
    "            - 我 --> 我\n",
    "            - 爱 -- > 我 | 爱\n",
    "            - 北京 --> 我 | 爱 | 北京\n",
    "            - 天安门 --> 我 | 爱 | 北京 | 天安门\n",
    "            - ！--> 我 | 爱 | 北京 | 天安门 | ！\n",
    "        - size：词的个数,  序列长度\n",
    "        - \n",
    "    \"\"\"\n",
    "    # 同一批次，任何一句话都是这样的规则，所以批量维度为 1 即可，计算时，自动广播\n",
    "    attn_shape = (1, size, size)\n",
    "    mask = torch.tril(torch.ones(attn_shape)).type(torch.bool)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2930259-9369-48d7-b087-24591b166a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    \"\"\"\n",
    "        Compute 'Scaled Dot Product Attention'\n",
    "        mask: [batch_size, 1, seq_len, seq_len]\n",
    "        计算 带缩放的点乘式注意力\n",
    "        \n",
    "    \"\"\"\n",
    "    # 取出最后一个维度，\n",
    "    # 特征维度 64\n",
    "    d_k = query.size(-1)\n",
    "    # [batch_size, h, seq_len, embed_dim] @ [batch_size, h, embed_dim, seq_len]\n",
    "    # [batch_size, h, seq_len, seq_len] - 原始点乘积\n",
    "    # 1， 计算了原始的点乘积\n",
    "    # 2，做了缩放\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    # \n",
    "    if mask is not None:\n",
    "        # 在 mask 为 零 的位置，填上 -1e9 很大的负数\n",
    "        scores = scores.masked_fill(mask = mask == False, value=-1e9)\n",
    "    # 求概率，得到最终的分数：\n",
    "    p_attn = scores.softmax(dim=-1)\n",
    "    # 如果有 dropout, 则应用 dropout\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    return torch.matmul(p_attn, value), p_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3812487b-65d5-4f75-9e6b-102bca172f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    \"\"\"\n",
    "        基于 PyTorch 设计自己的层：\n",
    "            - 参数？\n",
    "            - 逻辑？\n",
    "    \n",
    "        计算多头注意力\n",
    "            - 1，分成多头\n",
    "            - 2，按头计算注意力\n",
    "            - 3，合并最终的结果\n",
    "    \"\"\"\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        \"\"\"\n",
    "            h：头数  8\n",
    "            d_model: 向量的维度 512\n",
    "            - 单头形式：\n",
    "                - [batch_size, seq_len, embed_dim]\n",
    "            - 多头形式：\n",
    "                - [batch_size, h, seq_len, embed_dim // h]\n",
    "            \n",
    "        \"\"\"\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        # 向量维度必须能被头数整除\n",
    "        if d_model % h:\n",
    "            raise ValueError(\"向量维度 d_model 必须能被 头数 h 整除！\")\n",
    "        # We assume d_v always equals d_k\n",
    "        # 每一头的向量维度\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        # Q, K, V\n",
    "        # 核心参数\n",
    "        self.qkv_matrices = clones(nn.Linear(in_features=d_model, out_features=d_model, bias=False), 3)\n",
    "        # 多头特征合并之后的后处理\n",
    "        self.out = nn.Linear(in_features=d_model, out_features=d_model, bias=True)\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"\"\"\n",
    "            正向传播过程\n",
    "        \"\"\"\n",
    "        if mask is not None:\n",
    "            # Same mask applied to all h heads.\n",
    "            # [batch_size, seq_len, seq_len]\n",
    "            # [batch_size, 1, seq_len, seq_len]\n",
    "            mask = mask.unsqueeze(1)\n",
    "        # 取出批量 大小\n",
    "        nbatches = query.size(0)\n",
    "\n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k\n",
    "        # [batch_size, h, seq_len, embed_dim // h]\n",
    "        query, key, value = [\n",
    "            lin(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "            for lin, x in zip(self.qkv_matrices, (query, key, value))\n",
    "        ]\n",
    "\n",
    "        # 2) Apply attention on all the projected vectors in batch.\n",
    "        # x: [batch_size, h, seq_len, embed_dim // h]\n",
    "        # self.attn: [batch_size, h, seq_len, seq_len]\n",
    "        x, self.attn = attention(\n",
    "            query, key, value, mask=mask, dropout=self.dropout\n",
    "        )\n",
    "\n",
    "        # 3) \"Concat\" using a view and apply a final linear.\n",
    "        # x: [batch_size, h, seq_len, embed_dim // h] --> x: [batch_size, seq_len, h, embed_dim // h]\n",
    "        # --> x: [batch_size, seq_len, embed_dim]\n",
    "        x = (\n",
    "            x.transpose(1, 2)\n",
    "            .contiguous()\n",
    "            .view(nbatches, -1, self.h * self.d_k)\n",
    "        )\n",
    "        # 直接激活垃圾回收，立刻释放这些变量所占的空间\n",
    "        del query\n",
    "        del key\n",
    "        del value\n",
    "        # 合并之后，再做一次处理 \n",
    "        # [batch_size, seq_len, embed_dim] --> [batch_size, seq_len, embed_dim]\n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f41c9c4-5ae7-4ff9-9b8b-c4c8f08dd549",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "        Feed Forward \n",
    "        MLP: Multi-Layer Perceptron\n",
    "        Linear\n",
    "        - 功能：\n",
    "            - 把多头注意力层抽取的特征做一次后处理\n",
    "            - 把维度先升高，再下降\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        \"\"\"\n",
    "            d_model：原来的向量维度 512\n",
    "            d_ff：中间高维度 2048\n",
    "        \"\"\"\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.up_proj = nn.Linear(in_features=d_model, out_features=d_ff)\n",
    "        self.down_proj = nn.Linear(in_features=d_ff, out_features=d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.up_proj(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.down_proj(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66b85db2-8463-49e9-82bf-685a9cbd534d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    \"\"\"\n",
    "        自定义向量化层\n",
    "    \"\"\"\n",
    "    def __init__(self, dict_len, d_model):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.embed = nn.Embedding(num_embeddings=dict_len, embedding_dim=d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        x *= self.d_model ** 0.5\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f4d54811-f8b2-4584-a45b-17fafef80d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "        位置编码\n",
    "            - 给每个位置生成一个固定的向量\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Compute the positional encodings once in log space.\n",
    "        # [max_len, embed_dim]\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        # [1, max_len, embed_dim]\n",
    "        pe = pe.unsqueeze(0)\n",
    "        \n",
    "        # 注册缓冲区变量，model.state_dict()\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len, embed_dim]\n",
    "        # self.pe: [1, max_len, embed_dim]\n",
    "        # pe: [1, seq_len, embed_dim] \n",
    "        \n",
    "        # 根据实际序列长度取出 位置编码\n",
    "        pe = self.pe[:, : x.size(1), :].requires_grad_(False)\n",
    "        # 加上位置编码\n",
    "        x += pe\n",
    "        # dropout 处理\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a973e78-42dc-475e-bff9-3c873a6465ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建模型\n",
    "def make_model(src_dict_len, tgt_dict_len, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1):\n",
    "    \"\"\"\n",
    "        构建模型：\n",
    "            - src_dict_len: 输入侧字典的长度\n",
    "            - tgt_dict_len：输出侧字典的长度\n",
    "            - N：encoder 和 decoder 的重复次数\n",
    "            - d_model：模型内部，特征向量的维度\n",
    "            - d_ff：FF层中间的特征维度（先升维，再降维）\n",
    "            - h：注意力头数\n",
    "            - dropout：随机失活的概率\n",
    "    \"\"\"\n",
    "    c = copy.deepcopy\n",
    "    # 实例化了一个注意力层\n",
    "    attn = MultiHeadedAttention(h, d_model)\n",
    "    # 实例化一个前馈网络层\n",
    "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "    # 实例化一个位置编码对象\n",
    "    position = PositionalEncoding(d_model, dropout)\n",
    "\n",
    "    # 实例化模型\n",
    "    model = EncoderDecoder(\n",
    "        encoder=Encoder(layer=EncoderLayer(d_model, c(attn), c(ff), dropout), N=N),\n",
    "        decoder=Decoder(layer=DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N=N),\n",
    "        src_embed=nn.Sequential(Embedding(src_dict_len, d_model), position),\n",
    "        tgt_embed=nn.Sequential(Embedding(tgt_dict_len, d_model), position),\n",
    "        generator=Generator(d_model=d_model, dict_len=tgt_dict_len),\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526b2ea7-628d-419d-b4a2-a485c7fcb5b7",
   "metadata": {},
   "source": [
    "### 测试数据流"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b0ac2822-6b07-4179-89eb-ab6a468bc58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_test():\n",
    "    # 实例化一个model\n",
    "    model = make_model(src_dict_len=11, tgt_dict_len=22, N=2)\n",
    "    # 设置为 eval 模式\n",
    "    model.eval()\n",
    "    \n",
    "    src = torch.LongTensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]])\n",
    "    src_mask = torch.ones(1, 1, 10)\n",
    "\n",
    "    memory = model.encode(src, src_mask)\n",
    "    ys = torch.zeros(1, 1).type_as(src)\n",
    "\n",
    "    for i in range(9):\n",
    "        out = model.decode(\n",
    "            memory, src_mask, ys, subsequent_mask(ys.size(1)).type_as(src.data)\n",
    "        )\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.data[0]\n",
    "        ys = torch.cat(\n",
    "            [ys, torch.empty(1, 1).type_as(src.data).fill_(next_word)], dim=1\n",
    "        )\n",
    "\n",
    "    print(\"Example Untrained Model Prediction:\", ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "93a7d406-799e-4b3c-9f6f-522e72098ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Untrained Model Prediction: tensor([[ 0, 17, 16, 18, 14, 21, 12, 11,  4, 17]])\n"
     ]
    }
   ],
   "source": [
    "infer_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4ddf8e-e1d4-4ff3-be89-306f33e2d4dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(py311)",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
