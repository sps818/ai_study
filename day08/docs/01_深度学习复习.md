### 核心概念
- 1. deep learning
- 2. layer
  - 1. 线性层：linear layer / fully connected layer  / dense layer
      - 这一层做的就是相乘再相加，即求点积，英文是 inner product / dot product
      - 对每一个edge 都要乘上 weight（权重）
      - 对每一个output node 都要加上 bias（偏置）
  - 2. 激活层：activation layer / nonlinearity
      - 引入非线性因素，提高模型的表达能力
      - ReLU: Rectified Linear Unit
- 3. 模型 model:
    - 1. 由layer堆叠构成
    - 2. 承继 `nn.Module` 模块
    - 3. class 自定义一个类
      - 1. `__init__`
        - 接收和处理超参数
        - 实例化后续需要使用的层
      - 2. forward
        - 接收一个批量的特征
        - 根据算法逻辑，调用不同的层来处理数据
- 4. 训练 train
    - 1. 作必要的准备：
      - 1. model
      - 2. optimizer
      - 3. loss_fn
      - 4. data_loader
      - 5. epochs
    - 2. 流程：
      - 1. 遍历数据集加载器，取出一批数据
      - 2. 把特征 X 导入 模型，做正向传播，得到预测结果 y_pred
      - 3. 通过损失函数，计算预测结果和真实结果的误差 loss
      - 4. 根据 loss 做反向传播，计算每个参数的偏导数
      - 5. 利用优化器让每个参数减去偏导数，执行梯度下降的过程，优化一步
      - 6. 利用优化器清空每个参数的偏导数，为下次优化做准备
      - 7. 重复上面的过程，直到退出即可
- 5. 过程监控：
    - 1. 回归问题：
      - get_loss
    - 2. 分类问题：
      - get_acc
      - get_recall
      - get_precision
      - get_f1_score
- 6. 保存模型（分离式保存）
  - 1. 保存定义模型的类
  - 2. 保存模型训练完之后的权重

- 7. 数据的批量化打包
  - 目的：数据量比较大，不可能一次性加载到内存中！所以，需要分批次加载数据！！
  - 使用：
    - 1. 训练开始 
    - 2. 从硬盘中读取一批原始数据
    - 3. 做必要的预处理和数据增强 
    - 4. 交给模型训练
    - 5. 训练完成之后丢掉
    - 6. 从硬盘读取第二批数据
    - 7. ...
  - 本质：生成器 generator
  - PyTorch的策略：
    - 第1步：自定义 Dataset，继承 `torch.utils.data.Dataset`
      - 1. `__init__` 初始化工作，数据的地址
      - 2. `__len__` 实现查询数据集中的样本个数
      - 3. `__getitem__` 实现按索引读取样本
    - 第2步：实例化 DataLoader

### 表格类数据
- 机器学习
- 全连接网络
- 数据格式：[batch_size, num_features]
- [N, F]


### 图像/视频类数据？
- 图像：
  - 批量 N ：batch_size
  - 通道 C ：channel
  - 高度 H ：height
  - 宽度 W ：width

- 格式：
  - [batch_size, channel, height, width]
  - [批量、通道、高度、宽度]
  - [N, C, H, W]

- 视频：
  - 拆解为一张一张的图像即可
  - [batch_size, channel, height, width, time_step]
  - [批量、通道、高度、宽度、时间步]
  - [N, C, H, W, T]