{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56aac809-61f5-4567-9136-8cb5e92d0be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0e493d-b08e-4f20-af93-2244930e74dc",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4677157a-4a52-4f1f-8832-9f60e8467531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output的[N, C, H, W]：torch.Size([1, 2, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "# 构建一层卷积 - 模拟的是这张图：D:\\workspaces\\ai_study\\day08\\docs\\卷积2.gif\n",
    "conv1 = nn.Conv2d(in_channels=3, kernel_size=(3, 3), padding=1, stride=(2, 2), out_channels=2)\n",
    "# 模拟一个5*5的input , torch的结构是[N, C, H, W]\n",
    "X = torch.randn(1, 3, 5, 5)\n",
    "# 查看output\n",
    "print(f'Output的[N, C, H, W]：{conv1(X).shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fd7ff2d-a98d-4292-838f-fdef074c0e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "参数量：56\n"
     ]
    }
   ],
   "source": [
    "# 通过conv1.weight.shape方式查看到的内容是[C_out, C_in, K_H, K_W]\n",
    "C_out, C_in, K_H, K_W = conv1.weight.shape\n",
    "# 参数量计算 params = (C_in * K_H * K_W + 1) * C_out\n",
    "params_num = (C_in * K_H * K_W + 1) * C_out\n",
    "print(f'参数量：{params_num}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13460706-193c-47e5-8ae7-a252d4ff5a57",
   "metadata": {},
   "source": [
    "### BatchNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5460c709-8040-433b-994e-7e8eb3fb45b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"
     ]
    }
   ],
   "source": [
    "bn_layer = nn.BatchNorm2d(num_features=64)\n",
    "print(bn_layer)  # eps:防除零, momentum:移动平均动量（用于推理时的统计）, affine:是否学习gamma和beta, track_running_states:是否跟踪全局均值/方差（推理时使用）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e7e4fe-e7e7-4e14-bdac-83b0f9c2939a",
   "metadata": {},
   "source": [
    "### 搭建一个LeNet5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fea8e06c-a59f-4bc3-8199-eca77dc508fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b12f9195-8f6e-4dca-925a-52ce7aac8079",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet5Cover(nn.Module):\n",
    "    \"\"\"\n",
    "        自定义一个神经网络模型\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=1, output=10):\n",
    "        \"\"\"\n",
    "            初始化函数\n",
    "            \n",
    "            1. Flatten参数的默认值说明\n",
    "                nn.Flatten(start_dim = 1, end_dim = -1)默认值是1和-1. 意思是在[N, C, H, W]中，把从第1个维度开始到倒数第1个维度进行展平。\n",
    "                也就是[N, C, H, W]展平成[N, C * H * W]\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.C1 = nn.Conv2d(in_channels=in_channels, out_channels=6, kernel_size=5, stride=1, padding=0)\n",
    "        self.S2 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.C3 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1, padding=0)\n",
    "        self.S4 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.fllatten = nn.Flatten()\n",
    "        self.C5 = nn.Linear(in_features=400, out_features=120)\n",
    "        self.F6 = nn.Linear(in_features=120, out_features=84)\n",
    "        self.output = nn.Linear(in_features=84, out_features=output)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "            前向传播\n",
    "        \"\"\"\n",
    "        x = self.C1(x)\n",
    "        print(x.shape)\n",
    "        x = self.S2(x)\n",
    "        print(x.shape)\n",
    "        x = self.C3(x)\n",
    "        print(x.shape)\n",
    "        x = self.S4(x)\n",
    "        print(x.shape)\n",
    "        x = self.fllatten(x)\n",
    "        print(x.shape)\n",
    "        x = self.C5(x)\n",
    "        print(x.shape)\n",
    "        x = self.F6(x)\n",
    "        print(x.shape)\n",
    "        x = self.output(x)\n",
    "        print(x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d595d723-bb4a-469d-bdcd-05ee8826ca71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实例化模型对象\n",
    "lenet5 = LeNet5Cover(in_channels=1, output=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3656bff-50e4-4882-b67c-e05dacce2988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模拟一个Input的数据\n",
    "X = torch.randn(1, 1, 32, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "982462f7-7b1c-41e8-b10c-07682dd59780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 28, 28])\n",
      "torch.Size([1, 6, 14, 14])\n",
      "torch.Size([1, 16, 10, 10])\n",
      "torch.Size([1, 16, 5, 5])\n",
      "torch.Size([1, 400])\n",
      "torch.Size([1, 120])\n",
      "torch.Size([1, 84])\n",
      "torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "y_pred = lenet5.forward(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e6785d6-91ae-4326-9a76-861d1b6b140f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred shape:tensor([[-0.1017, -0.0560, -0.1336, -0.0871, -0.1800, -0.0230, -0.1144,  0.1432,\n",
      "         -0.1151,  0.0446]], grad_fn=<AddmmBackward0>)\n",
      "shape is torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "print(f'y_pred shape:{y_pred}\\nshape is {y_pred.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3262c360-10eb-4952-8b92-ce4e645411bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1017, -0.0560, -0.1336, -0.0871, -0.1800, -0.0230, -0.1144,  0.1432,\n",
       "         -0.1151,  0.0446]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "432519a6-980e-44d4-bacf-86640ce42345",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\63447\\AppData\\Local\\Temp\\ipykernel_13500\\2470479642.py:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:494.)\n",
      "  y_pred.grad\n"
     ]
    }
   ],
   "source": [
    "y_pred.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f748bab-7127-4787-b1d1-7bd0cf31a7a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LeNet5Cover(\n",
       "  (C1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (S2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (C3): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (S4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fllatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (C5): Linear(in_features=400, out_features=120, bias=True)\n",
       "  (F6): Linear(in_features=120, out_features=84, bias=True)\n",
       "  (output): Linear(in_features=84, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lenet5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "325ff989-85ad-4077-b5c8-24b6ce7d1b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet5Cover_Gai(nn.Module):\n",
    "    \"\"\"\n",
    "        自定义一个神经网络模型 - 对上面的神经网络进行了改良.\n",
    "        改良：做了流水线\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=1, output=10):\n",
    "        \"\"\"\n",
    "            初始化函数\n",
    "            \n",
    "            1. Flatten参数的默认值说明\n",
    "                nn.Flatten(start_dim = 1, end_dim = -1)默认值是1和-1. 意思是在[N, C, H, W]中，把从第1个维度开始到倒数第1个维度进行展平。\n",
    "                也就是[N, C, H, W]展平成[N, C * H * W]\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.pipline = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=6, kernel_size=5, stride=1, padding=0),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0),\n",
    "            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1, padding=0),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=400, out_features=120),\n",
    "            nn.Linear(in_features=120, out_features=84),\n",
    "            nn.Linear(in_features=84, out_features=output)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "            前向传播\n",
    "        \"\"\"\n",
    "        return self.pipline(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "11efb3e1-9914-4dd2-95af-687041225582",
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet5_gai = LeNet5Cover_Gai()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3db79174-44a6-4fbf-90cf-8d8ddfe5595c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.randn(1, 1, 32, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bd50270c-66eb-44ab-a812-c3ccce10f26b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10])\n",
      "LeNet5Cover_Gai(\n",
      "  (pipline): Sequential(\n",
      "    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Flatten(start_dim=1, end_dim=-1)\n",
      "    (5): Linear(in_features=400, out_features=120, bias=True)\n",
      "    (6): Linear(in_features=120, out_features=84, bias=True)\n",
      "    (7): Linear(in_features=84, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "y_pred = lenet5_gai.forward(X)\n",
    "print(y_pred.shape)\n",
    "print(lenet5_gai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed9e923-c9b3-4658-94ec-471740571f6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(py311)",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
