### 1.迭代法
- 适合场景：
	- `不能一次搞定`的问题！！！
	- 分成`很多步`来逐步解决！！！
	- KMeans 聚类算法
	- Linear/Logistic Regression  线性回归和逻辑回归
- 三个关键点：
	- 1. `随机` 的开始（random，rough start）
		- 随机 = 普适
	
	- 2. `逐步变好`的策略（optimize）
		- step by step
		- 每天进步一天天
		- 相信累积，相信长期主义
	
	- 3. `退出`条件(stop)
		- 固定步数
		- 误差限制

- KMeans算法：
	- 使用最多、最简单易用的聚类算法！！！
	- clustering 聚类
	- 本质：无标签的分类
	- 没有标签！只有特征！
	- 只根据特征来进行分类！
	- 原来没有类别，根据业务需要把样本分成几类！
	- 思想：物以类聚人以群分（跟KNN是一样的）
		- 挨得近的看作一类，挨得远的看作另一类
	- KMeans的内涵：
		- K：代表分类数量，K个类
		- Means:
			- Mean 均值
			- s 多次
	- 迭代式算法跟随机的出生点是有关系的，存在一个小概率的不稳定！
			


### 在数据科学中，生成假数据 fake data 是一种重要的能力！！！


### 线性回归
- Linear：
	- 自变量和因变量都是一次方关系
	- 因变量是自变量的线性组合
		- 每个自变量都乘上一个权重，再加在一起
			- weight w
			- 乘到变量上，代表该变量的重要程度
			- 有多少个变量，就有多少个权重
		- 最后加上一个公共的偏置
			- bias b
			- 加到最终结果上
			- 有多少个最终的结果，就有多少个偏置	
	- $y = wx + b$
	- $f(x, y) = w_1x + w_2y + b$
	
- Regression：
	- 预测连续型数据

- 房价预测：
	- 线性回归：
		- 假定：房价是由特征们的线性组合得到的
		
		
- 算法流程：
	- 1. 随机初始化 w 和 b：
		- 假设函数/模型：
			- $y = w_1x_1 + w_2x_2 + ... w_{13}x_{13} + b$
	- 2. 从训练集中，取出一批样本 batch_X, batch_y:
		- 把特征 batch_X 带入模型，得到一个预测结果 y_pred
			- 此时：y_pred 是 w 和 b 的函数
		- 衡量预测结果和真实结果的误差
			- loss = loss_fn(y_pred, batch_y)
			- 预测的越差，误差越大；预测的越好，误差就越小；
			- loss 是 y_pred 的函数，y_pred 又是 w 和 b 的函数
			- loss 是 w 和 b 的函数
			- 误差的大小是受 w 和 b 的影响
		- 模型变好，误差变小：
			- 数学问题：
				- 求函数的最/极小值
				- 当 w 和 b 是多少的时候，loss 可以取得最小值？
		- 综上所述，模型优化的问题，就变成了一个函数求最小值的问题！

### 如何求函数的最小值？
- 求 y = F(x) 的最小值？

- 理论数学:
	- 1. 求导数/偏导
	- 2. 令导数/偏导等于零
	- 3. 解方程/组，得到疑似结果，再做一进步结果验证
	（样本不在模型上，在模型周围！！！）
	
- 工程上：
	- 迭代法
	- (随机)梯度下降法 SGD  Stochastic Gradient Descent
	


				
				
		
			
		
		
	
	