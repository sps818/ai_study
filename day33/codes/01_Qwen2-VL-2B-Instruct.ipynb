{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7231945e-3b12-4587-a086-c5aa4ae94be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Qwen2VLForConditionalGeneration\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ccc6113-6822-4524-b5df-7629da84f6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置可以使用的卡\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0, 1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e56261a-1779-4095-8adf-04b94b02746b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置模型地址\n",
    "model_dir = \"Qwen2-VL-2B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8e8e32b-9911-4652-99a3-0d1ab56128e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcd356d5af444e08960d6cc38d605ea0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 加载模型\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    model_dir, \n",
    "    torch_dtype=\"auto\", \n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65953222-c645-4c9f-9edc-d14d806a3cb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2VLForConditionalGeneration(\n",
       "  (visual): Qwen2VisionTransformerPretrainedModel(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)\n",
       "    )\n",
       "    (rotary_pos_emb): VisionRotaryEmbedding()\n",
       "    (blocks): ModuleList(\n",
       "      (0-31): 32 x Qwen2VLVisionBlock(\n",
       "        (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): VisionSdpaAttention(\n",
       "          (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "          (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (mlp): VisionMlp(\n",
       "          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (act): QuickGELUActivation()\n",
       "          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (merger): PatchMerger(\n",
       "      (ln_q): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=5120, out_features=1536, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (model): Qwen2VLModel(\n",
       "    (embed_tokens): Embedding(151936, 1536)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen2VLDecoderLayer(\n",
       "        (self_attn): Qwen2VLSdpaAttention(\n",
       "          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
       "          (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
       "          (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "          (rotary_emb): Qwen2VLRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
       "          (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
       "          (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2VLRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1536, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9cd4085a-e5bd-4a9b-ad2a-0c94d6ced2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "nums = 0\n",
    "for param in model.visual.parameters():\n",
    "    nums += param.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31bc06b1-9f7b-4f8c-8a0e-e729e6d57d13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "665271296"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "226c506f-8a33-4914-83b3-00411a4d06b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1543714304"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums = 0\n",
    "for param in model.model.parameters():\n",
    "    nums += param.numel()\n",
    "nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12efe9d2-6875-46d9-9e44-8b655da14c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default processer\n",
    "processor = AutoProcessor.from_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01aada80-a463-4daa-bf6c-20bd7f426798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2VLProcessor:\n",
       "- image_processor: Qwen2VLImageProcessor {\n",
       "  \"do_convert_rgb\": true,\n",
       "  \"do_normalize\": true,\n",
       "  \"do_rescale\": true,\n",
       "  \"do_resize\": true,\n",
       "  \"image_mean\": [\n",
       "    0.48145466,\n",
       "    0.4578275,\n",
       "    0.40821073\n",
       "  ],\n",
       "  \"image_processor_type\": \"Qwen2VLImageProcessor\",\n",
       "  \"image_std\": [\n",
       "    0.26862954,\n",
       "    0.26130258,\n",
       "    0.27577711\n",
       "  ],\n",
       "  \"max_pixels\": 12845056,\n",
       "  \"merge_size\": 2,\n",
       "  \"min_pixels\": 3136,\n",
       "  \"patch_size\": 14,\n",
       "  \"processor_class\": \"Qwen2VLProcessor\",\n",
       "  \"resample\": 3,\n",
       "  \"rescale_factor\": 0.00392156862745098,\n",
       "  \"size\": {\n",
       "    \"max_pixels\": 12845056,\n",
       "    \"min_pixels\": 3136\n",
       "  },\n",
       "  \"temporal_patch_size\": 2\n",
       "}\n",
       "\n",
       "- tokenizer: Qwen2TokenizerFast(name_or_path='Qwen2-VL-2B-Instruct', vocab_size=151643, model_max_length=32768, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151646: AddedToken(\"<|object_ref_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151647: AddedToken(\"<|object_ref_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151648: AddedToken(\"<|box_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151649: AddedToken(\"<|box_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151650: AddedToken(\"<|quad_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151651: AddedToken(\"<|quad_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151652: AddedToken(\"<|vision_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151653: AddedToken(\"<|vision_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151654: AddedToken(\"<|vision_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151655: AddedToken(\"<|image_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151656: AddedToken(\"<|video_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")\n",
       "\n",
       "{\n",
       "  \"processor_class\": \"Qwen2VLProcessor\"\n",
       "}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39d75947-f36d-4239-9f9c-7ccfd4884620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建聊天消息\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"你是一个有用的助手！\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \n",
    "             \"text\": \"请描述一下这张图像\"},\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image\": \"ViT.png\",\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61157071-054d-4283-8356-2e135532f71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparation for inference\n",
    "text = processor.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "192d4811-eb84-4220-99db-76bd343f7dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "你是一个有用的助手！<|im_end|>\n",
      "<|im_start|>user\n",
      "请描述一下这张图像<|vision_start|><|image_pad|><|vision_end|><|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce73d851-c902-4093-8248-3e843031abb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理视频和图像\n",
    "image_inputs, video_inputs = process_vision_info(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "06883f4f-8c63-47f9-abbb-d7c8345a2d75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<PIL.Image.Image image mode=RGB size=1428x728>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "15ac8552-c7a3-42ab-b4c6-d79047fc2d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c7eb760a-5033-4981-b8f9-d394860a7ce8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'pixel_values', 'image_grid_thw'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f6ea031a-16d2-44be-9fd4-96ff1a7b66be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6237504"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.pixel_values.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "81b19df1-eded-4cd3-9738-c8b4a10772df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  1,  52, 102]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.image_grid_thw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d2151365-1719-4439-8f29-14602b0e862c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19612"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "98 * 146 + 52 * 102"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1162cb4f-1758-475f-b2d6-62bfc72c85fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = inputs.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d3a175c8-d708-4b92-8fa1-d95ecec6926f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference: Generation of the output\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b942ed2c-d4b9-4ffe-a5a5-9214cf24d871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['这张图像展示了视觉Transformer（ViT）模型的结构。ViT是一种用于图像分类和视觉理解的神经网络模型，它结合了Transformer和卷积神经网络（CNN）的优点。\\n\\n### 左侧部分\\n- **输入层**：图像作为输入，通过一个分类头（MLP Head）进行分类。\\n- **Transformer Encoder**：ViT的核心部分，它使用Transformer架构对输入图像进行编码。\\n- **Patch + Position Embedding**：图像被分割成多个矩形区域（patch）并附加位置编码，用于捕捉图像的局部特征。\\n- **Linear Projection of Flattened Patches**：将每个patch的特征投影到一个共享的特征空间中。\\n- **Transformer Encoder**：使用Transformer架构对投影后的特征进行编码，以捕捉图像的全局结构和特征。\\n\\n### 右侧部分\\n- **Transformer Encoder**：与左侧部分相同，但使用了不同的结构和组件。\\n- **Multi-Head Attention**：用于处理不同位置的特征之间的交互。\\n- **Norm**：用于正则化和缩放输出。\\n- **Embedded Patches**：将图像分割成嵌入的特征向量。\\n\\n### 结论\\nViT通过将Transformer架构与CNN结合，能够有效地处理图像的局部和全局特征，从而在图像分类和视觉理解任务中表现出色。']\n"
     ]
    }
   ],
   "source": [
    "# 删掉生成部分\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "# 解码输出\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75ac68b-1976-4cfb-a043-9eaacfabe3fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabde93a-34b8-4964-9dfb-ec83181ce080",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a648fd6-15aa-4381-b1d8-52f07bfa7876",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
