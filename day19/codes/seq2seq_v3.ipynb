{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a519495-7b12-4c81-97ec-d5888b6121ae",
   "metadata": {},
   "source": [
    "### 1. 读取原始数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a828c837-1cc7-49c1-b5cd-5e38ce6e99ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "import joblib\n",
    "import jieba\n",
    "import opencc\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18c7164-9af5-4137-9a0a-57983875d979",
   "metadata": {},
   "source": [
    "### 2. 构建分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f04d8037-12c5-4a5e-a620-bf1a0fd8c472",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer(object):\n",
    "    \"\"\"\n",
    "        自定义一个分词器，实现基本功能：\n",
    "            - 1. 根据输入的语料，构建字典\n",
    "            - 2. 输入src的句子，输出对应的id\n",
    "            - 3. 输入tgt的句子，输出对应的id\n",
    "            - 4. 输入tgt的id，输出tgt的句子\n",
    "    \"\"\"\n",
    "    def __init__(self, data_file):\n",
    "        \"\"\"\n",
    "            分词器初始化\n",
    "                - 默认：根据输入的语料，构建字典\n",
    "        \"\"\"\n",
    "        self.data_file = data_file\n",
    "        # 输入侧 src --> source\n",
    "        self.src_token2idx = None\n",
    "        self.src_idx2token = None\n",
    "        # 输出侧 tgt --> target\n",
    "        self.tgt_token2idx = None\n",
    "        self.tgt_idx2token = None\n",
    "        # 构建字典\n",
    "        self._build_dict()\n",
    "    \n",
    "    def _build_dict(self):\n",
    "        \"\"\"\n",
    "            构建字典\n",
    "        \"\"\"\n",
    "        if self.src_token2idx:\n",
    "            print(\"字典已经构建过了\")\n",
    "            return\n",
    "        elif os.path.exists(os.path.join(\".cache\", \"dicts.lxh\")):\n",
    "            print(\"从缓存中读取字典\")\n",
    "            self.src_token2idx, self.src_idx2token, self.tgt_token2idx, self.tgt_idx2token = joblib.load(filename=os.path.join(\".cache\", \"dicts.lxh\"))\n",
    "            return\n",
    "        \n",
    "        # 从零构建字典\n",
    "        data = pd.read_csv(filepath_or_buffer=self.data_file, sep=\"\\t\", header=None)\n",
    "        data.columns = [\"src\", \"tgt\"]\n",
    "        rows, cols  = data.shape\n",
    "        # 构建词典\n",
    "        src_tokens = {\"<UNK>\", \"<PAD>\", \"<SOS>\", \"<EOS>\"}\n",
    "        tgt_tokens = {\"<UNK>\", \"<PAD>\", \"<SOS>\", \"<EOS>\"}\n",
    "        for row_idx in tqdm(range(rows)):\n",
    "            src, tgt = data.loc[row_idx, :]\n",
    "            src_tokens.update(set(self.split_english_sentence(src)))\n",
    "            tgt_tokens.update(set(self.split_chinese_sentence(tgt)))\n",
    "        \n",
    "        # 构建 src 的 字典\n",
    "        self.src_token2idx = {token: idx for idx, token in enumerate(src_tokens)}\n",
    "        self.src_idx2token = {idx: token for token, idx in self.src_token2idx.items()}\n",
    "\n",
    "        # 构建 tgt 的 字典\n",
    "        self.tgt_token2idx = {token: idx for idx, token in enumerate(tgt_tokens)}\n",
    "        self.tgt_idx2token = {idx: token for token, idx in self.tgt_token2idx.items()}\n",
    "\n",
    "        # 保存\n",
    "        dicts = [self.src_token2idx, self.src_idx2token, self.tgt_token2idx, self.tgt_idx2token]\n",
    "        joblib.dump(value=dicts, filename=os.path.join(\".cache\", \"dicts.lxh\"))\n",
    "        \n",
    "    def split_english_sentence(self, sentecne):\n",
    "        \"\"\"\n",
    "            英文句子切分\n",
    "        \"\"\"\n",
    "        sentecne = sentecne.strip()\n",
    "        # 小写\n",
    "        tokens = [token for token in jieba.lcut(sentecne.lower()) if token not in (\"\", \" \", \"'\")]\n",
    "        return tokens\n",
    "    \n",
    "    def split_chinese_sentence(self, sentence):\n",
    "        \"\"\"\n",
    "            中文句子切分\n",
    "        \"\"\"\n",
    "        # 实例化一个繁体转简体的工具\n",
    "        converter = opencc.OpenCC(config=\"t2s\")\n",
    "        sentence = converter.convert(text=sentence)\n",
    "        # 分词\n",
    "        tokens = [token for token in jieba.lcut(sentence) if token not in [\"\", \" \"]]\n",
    "        return tokens\n",
    "        \n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "            返回必要的打印信息\n",
    "        \"\"\"\n",
    "        if self.src_token2idx:\n",
    "            out = f\"Tokenizer: [src: {len(self.src_token2idx)}, tgt: {len(self.tgt_token2idx)}]\"\n",
    "        else:\n",
    "            out = f\"尚无字典信息\"\n",
    "        return out\n",
    "    \n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "            返回必要的打印信息\n",
    "        \"\"\"\n",
    "        return self.__str__()\n",
    "\n",
    "    def encode_src(self, src_sentence, src_max_len):\n",
    "        \"\"\"\n",
    "            把分词后的句子，变成 id\n",
    "                - 按本批次的最大长度来填充\n",
    "                - src 不加 special token\n",
    "                    - EOS\n",
    "                    - SOS\n",
    "        \"\"\"\n",
    "        # 转换\n",
    "        src_idx = [self.src_token2idx.get(token, self.src_token2idx.get(\"<UNK>\")) for token in src_sentence]\n",
    "        # 填充\n",
    "        src_idx = (src_idx + [self.src_token2idx.get(\"<PAD>\")] * src_max_len)[:src_max_len]\n",
    "        return src_idx\n",
    "\n",
    "    def encode_tgt(self, tgt_sentence, tgt_max_len):\n",
    "        \"\"\"\n",
    "            把分词后的tgt句子变成 id\n",
    "                - <SOS>, 我, 爱, 北京, 天安门, ！, <EOS>, <PAD>, <PAD>\n",
    "        \"\"\"\n",
    "        # 前后加上 special token\n",
    "        tgt_sentence = [\"<SOS>\"] + tgt_sentence + [\"<EOS>\"]\n",
    "        tgt_max_len += 2\n",
    "        tgt_idx = [self.tgt_token2idx.get(token, self.tgt_token2idx.get(\"<UNK>\")) for token in tgt_sentence]\n",
    "        tgt_idx = (tgt_idx + [self.tgt_token2idx.get(\"<PAD>\")] * tgt_max_len)[:tgt_max_len]\n",
    "        return tgt_idx\n",
    "    \n",
    "    def decode_src(self, src_ids):\n",
    "        \"\"\"\n",
    "            把生成的id序列转换为token\n",
    "        \"\"\"\n",
    "        outs = []\n",
    "        # [batch_size, seq_len]\n",
    "        for temp_src in src_ids:\n",
    "            outs.append([self.src_idx2token.get(src_id, \"<UNK>\") for src_id in temp_src if src_id != tokenizer.src_token2idx.get(\"<PAD>\")])\n",
    "        return outs\n",
    "        \n",
    "    def decode_tgt(self, tgt_ids):\n",
    "        \"\"\"\n",
    "            把生成的id序列转换为token\n",
    "        \"\"\"\n",
    "        y_preds = []\n",
    "        # [batch_size, seq_len]\n",
    "        for temp_tgt in tgt_ids:\n",
    "            y_preds.append([self.tgt_idx2token.get(tgt_id) for tgt_id in temp_tgt if tgt_id not in(tokenizer.tgt_token2idx.get(\"<EOS>\"),\n",
    "                                                                                                   tokenizer.tgt_token2idx.get(\"<SOS>\"),\n",
    "                                                                                                   tokenizer.tgt_token2idx.get(\"<PAD>\"))])\n",
    "        return y_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64092dda-96ab-4f86-aabb-945f2bad26cf",
   "metadata": {},
   "source": [
    "### 3. 数据打包\n",
    "- 既要又要\n",
    "    - 既要批量化训练\n",
    "    - 又要消除填充PAD的噪声污染\n",
    "- collate_fn\n",
    "    - 手动排序！！！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e34ac30c-7758-46b0-b479-633dbb5e246b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import pandas\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b8358a9-f212-4362-8213-26a3b12110b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "从缓存中读取字典\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(data_file=\"data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b501b407-b66d-4ad3-a2cc-48f86120a9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqDataset(Dataset):\n",
    "    \"\"\"\n",
    "        自定义数据集\n",
    "    \"\"\"\n",
    "    def __init__(self, data_file, part=\"train\", tokenizer=tokenizer):\n",
    "        \"\"\"\n",
    "            初始化\n",
    "        \"\"\"\n",
    "        self.data_file = data_file\n",
    "        self.tokenier = tokenizer\n",
    "        self.part = part\n",
    "        self.data = None\n",
    "        self._load_data()\n",
    "\n",
    "    def _load_data(self):\n",
    "        \"\"\"\n",
    "            加载数据\n",
    "        \"\"\"\n",
    "        if self.data:\n",
    "            print(\"数据集已经构建过了\")\n",
    "            return\n",
    "        elif os.path.exists(os.path.join(\".cache\", \"data.lxh\")):\n",
    "            print(\"从缓存中读取数据\")\n",
    "            # 原始数据\n",
    "            data = joblib.load(filename=os.path.join(\".cache\", \"data.lxh\"))\n",
    "            # 80% 训练集\n",
    "            # 20% 测试集\n",
    "            nums = int(len(data) * 0.80)\n",
    "            self.data = data[:nums] if self.part == \"train\" else data[nums:]\n",
    "            return\n",
    "        # 从零读取\n",
    "        data = pd.read_csv(filepath_or_buffer=self.data_file, sep=\"\\t\", header=None)\n",
    "        # shuffle\n",
    "        data = data.sample(frac=1).to_numpy()\n",
    "        # 保存数据\n",
    "        joblib.dump(value=data, filename=os.path.join(\".cache\", \"data.lxh\"))\n",
    "        # 数据截取\n",
    "        nums = int(len(data) * 0.80)\n",
    "        self.data = data[:nums] if self.part == \"train\" else data[nums:]\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "            通过索引来访问样本\n",
    "        \"\"\"\n",
    "        src, tgt = self.data[idx]\n",
    "        src = tokenizer.split_english_sentence(src)\n",
    "        tgt = tokenizer.split_chinese_sentence(tgt)\n",
    "        return src, len(src), tgt, len(tgt)\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "            返回该数据集的样本个数\n",
    "        \"\"\"\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1e8b153-9801-47a0-bafb-dbf42bcdb38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch, tokenizer=tokenizer):\n",
    "    \"\"\"\n",
    "        回调函数\n",
    "            - src 样本，按照长度逆序排列\n",
    "    \"\"\"\n",
    "    # 按 src_len 逆序\n",
    "    batch = sorted(batch, key=lambda ele: ele[1], reverse=True)\n",
    "    # 分拆成4个集合\n",
    "    src_sentences, src_lens, tgt_sentences, tgt_lens = zip(*batch)\n",
    "    # 1. src 转 id\n",
    "    src_max_len = src_lens[0]\n",
    "    src_idxes = []\n",
    "    for src_sentence in src_sentences:\n",
    "        src_idxes.append(tokenizer.encode_src(src_sentence, src_max_len))\n",
    "\n",
    "    # 2. tgt 转 id\n",
    "    tgt_max_len = max(tgt_lens)\n",
    "    tgt_idxes = []\n",
    "    for tgt_sentence in tgt_sentences:\n",
    "        tgt_idxes.append(tokenizer.encode_tgt(tgt_sentence, tgt_max_len))\n",
    "\n",
    "    # 所有数据转张量 torch.long\n",
    "    # [src_max_len, batch_size]\n",
    "    src_idxes = torch.tensor(data=src_idxes, dtype=torch.long).t()\n",
    "    # (batch_size, )\n",
    "    src_lens = torch.tensor(data=src_lens, dtype=torch.long)\n",
    "    # [tgt_max_len + 2, batch_size]\n",
    "    tgt_idxes = torch.tensor(data=tgt_idxes, dtype=torch.long).t()\n",
    "    # (batch_size, )\n",
    "    tgt_lens = torch.tensor(data=tgt_lens, dtype=torch.long)\n",
    "\n",
    "    return src_idxes, src_lens, tgt_idxes, tgt_lens\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a53f58c-7829-4c02-ad7a-26c94ae32e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "从缓存中读取数据\n",
      "从缓存中读取数据\n"
     ]
    }
   ],
   "source": [
    "# 训练集\n",
    "train_dataset = Seq2SeqDataset(data_file=\"data.txt\", part=\"train\")\n",
    "train_dataloader = DataLoader(dataset=train_dataset, \n",
    "                              shuffle=True, \n",
    "                              batch_size=32,\n",
    "                              collate_fn=collate_fn)\n",
    "# 测试集\n",
    "test_dataset = Seq2SeqDataset(data_file=\"data.txt\", part=\"test\")\n",
    "test_dataloader = DataLoader(dataset=test_dataset, \n",
    "                              shuffle=False, \n",
    "                              batch_size=32,\n",
    "                              collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3af8b0c0-ea74-4158-8e0d-7d270349404e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.tgt_idx2token.get(3621)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "546046fd-100e-40af-8063-32b11e7f8708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for src, src_lens, tgt, tgt_lens in test_dataloader:\n",
    "#     print(src.shape, src_lens.shape, tgt, tgt_lens.shape)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386b6164-934b-4910-8756-a8504dccdd4c",
   "metadata": {},
   "source": [
    "### 4. 编码器设计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6baba9f0-b8bd-4957-9021-550b0e2d5c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4163599-b557-4b6f-b670-1f8457e9140a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "        自定义一个编码器，处理 src\n",
    "            - `Seq` 2 Seq\n",
    "            - 只是 一个很单纯 的 RNN\n",
    "            - 没有任何的差别\n",
    "    \"\"\"\n",
    "    def __init__(self, num_embeddings=len(tokenizer.src_token2idx), embedding_dim=256):\n",
    "        # 仅用于上坟，没有任何其他作用！\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(num_embeddings=num_embeddings,\n",
    "                                 embedding_dim=embedding_dim, \n",
    "                                 padding_idx=tokenizer.src_token2idx.get(\"<PAD>\"))\n",
    "        self.gru = nn.GRU(input_size=embedding_dim, \n",
    "                          hidden_size=embedding_dim)\n",
    "\n",
    "    def forward(self, src, src_lens):\n",
    "        \"\"\"\n",
    "            前向传播\n",
    "                - 消除 PAD 影响\n",
    "        \"\"\"\n",
    "        # [src_max_len, batch_size] --> [src_max_len, batch_size, embed_dim]\n",
    "        src = self.embed(src)\n",
    "        # 压紧被填充的序列\n",
    "        src = nn.utils.rnn.pack_padded_sequence(input=src, lengths=src_lens, batch_first=False)\n",
    "        out, hn = self.gru(src)\n",
    "        return hn[0, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7fccba3-02bf-4d6c-9b61-d44da45166f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db95be48-8de6-4163-85d3-1a6b2b493342",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\63447\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.918 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 32])\n",
      "torch.Size([32, 256])\n"
     ]
    }
   ],
   "source": [
    "for src, src_lens, tgt, tgt_lens in train_dataloader:\n",
    "    print(src.shape)\n",
    "    context = encoder(src, src_lens)\n",
    "    print(context.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577d494a-f1e2-4659-bf1a-31a6ca5a9196",
   "metadata": {},
   "source": [
    "### 5. 解码器设计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "437503e5-833b-4557-b02b-3a81af40efcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78c0f809-495e-4288-be4b-4507e061191a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "        实现解码器：\n",
    "            - 训练时：\n",
    "                - 考虑 teacher forcing\n",
    "            - 推理时：\n",
    "                - 考虑 自回归\n",
    "    \"\"\"\n",
    "    def __init__(self, num_embeddings=len(tokenizer.tgt_token2idx), embedding_dim=256):\n",
    "        super().__init__()\n",
    "        # 向量化的过程\n",
    "        self.embed = nn.Embedding(num_embeddings=num_embeddings, \n",
    "                                  embedding_dim=embedding_dim, \n",
    "                                  padding_idx=tokenizer.tgt_token2idx.get(\"<PAD>\"))\n",
    "        \n",
    "        # 手动挡，分步特征抽取，实现自回归逻辑！！！\n",
    "        self.gru_cell = nn.GRUCell(input_size=embedding_dim,\n",
    "                                  hidden_size=embedding_dim)\n",
    "        \n",
    "        # 输出 embed_dim --> dict_len\n",
    "        self.out = nn.Linear(in_features=embedding_dim, out_features=len(tokenizer.tgt_token2idx))\n",
    "    \n",
    "    def forward(self, context, tgt):\n",
    "        \"\"\"\n",
    "            训练时的正向推理：\n",
    "\n",
    "                context: 上下文向量，中间表达\n",
    "                tgt：标签\n",
    "                tgt_lens：生成的句子的有效长度（不包含 <SOS>和<EOS>）     \n",
    "        \"\"\"\n",
    "        # 生成侧的输入 \n",
    "        tgt_input = tgt[:-1, :]\n",
    "        # 生成侧的输出\n",
    "        tgt_output = tgt[1:, :]\n",
    "        # 实际步长，批量大小\n",
    "        SEQ_LEN, BATCH_SIZE = tgt_input.shape\n",
    "        # 准备初始状态\n",
    "        hn = context\n",
    "        # 有多少步，就循环多少次\n",
    "        outs = []\n",
    "        # [SOS]\n",
    "        # batch_size, embed_dim\n",
    "        step_input = self.embed(tgt_input[0, :].view(1, -1))[0, :, :]\n",
    "        \n",
    "        for step in range(SEQ_LEN):\n",
    "            # print(f\"第 {step} 步，总共 {SEQ_LEN} 步\")\n",
    "            # 正向传播\n",
    "            hn = self.gru_cell(step_input, hn)\n",
    "            # 生成结果\n",
    "            y_pred = self.out(hn)\n",
    "            # 保留所有生成的结果（做交叉熵损失用）\n",
    "            outs.append(y_pred)\n",
    "            # 为下一轮做准备\n",
    "            if step < SEQ_LEN - 1:\n",
    "                # 训练时采用 50% 的概率去使用 teacher forcing 优化策略\n",
    "                teacher_forcing = random.random() >= 0.5\n",
    "                if teacher_forcing:\n",
    "                    # 如果采用 teacher_forcing，则需要输入标准答案\n",
    "                    step_input = self.embed(tgt_input[step + 1, :].view(1, -1))[0, :, :]\n",
    "                else:\n",
    "                    # 如果不采用 teacher_forcing, 则 输入上一次生成的结果\n",
    "                    y_pred = y_pred.argmax(dim=-1, keepdim=True).view(1, -1)\n",
    "                    step_input = self.embed(y_pred)[0, :, :]\n",
    "        return torch.stack(tensors=outs, dim=0)\n",
    "    \n",
    "    def infer(self, context, max_new_tokens=200):\n",
    "        \"\"\"\n",
    "            推理专用\n",
    "                - context: 上句的中间表达\n",
    "                - max_new_tokens：为最大生成长度\n",
    "        \"\"\"\n",
    "        # 准备初始状态\n",
    "        hn = context\n",
    "        # 有多少步，就循环多少次\n",
    "        outs = []\n",
    "        # [SOS]\n",
    "        # batch_size, embed_dim\n",
    "        BATCH_SIZE, _ = context.shape\n",
    "        # [seq_len, batch_size] --> [seq_len, batch_size, embed_dim]\n",
    "        step_input = self.embed(torch.tensor(data=[tokenizer.tgt_token2idx.get(\"<SOS>\")] * BATCH_SIZE, \n",
    "                                             dtype=torch.long, \n",
    "                                             device=device\n",
    "                                            ).view(1,  -1))[0, :, :]\n",
    "        for step in range(max_new_tokens):\n",
    "            # print(f\"第 {step} 步，总共 {max_new_tokens} 步\")\n",
    "            # 正向传播\n",
    "            hn = self.gru_cell(step_input, hn)\n",
    "            # 生成结果\n",
    "            y_pred = self.out(hn)\n",
    "            # 保留所有生成的结果（做交叉熵损失用）\n",
    "            outs.append(y_pred)\n",
    "            # 为下一轮做准备\n",
    "            if step < max_new_tokens - 1:\n",
    "                # 如果不采用 teacher_forcing, 则 输入上一次生成的结果\n",
    "                # 贪心解码（略显僵硬，不够圆滑）\n",
    "                y_pred = y_pred.argmax(dim=-1, keepdim=True).view(1, -1)\n",
    "                step_input = self.embed(y_pred)[0, :, :]\n",
    "        return torch.stack(tensors=outs, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d71afc3-7c33-4a03-a11c-67c2fc442265",
   "metadata": {},
   "source": [
    "### 6. 完整模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c28eb50d-a2eb-4131-87cb-055f478694ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    \"\"\"\n",
    "        定义一个完整模型\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "        \n",
    "    def forward(self, src, src_lens, tgt):\n",
    "        \"\"\"\n",
    "            训练时使用的前向传播\n",
    "        \"\"\"\n",
    "        context = self.encoder(src, src_lens)\n",
    "        outs = self.decoder(context, tgt)\n",
    "        return outs\n",
    "\n",
    "    def infer(self, src, src_lens, max_new_tokens=128):\n",
    "        \"\"\"\n",
    "            推理过程\n",
    "                - 批量的\n",
    "                - src 批量的句子\n",
    "                - src_lens 每个句子的长度\n",
    "                \n",
    "        \"\"\"\n",
    "        # 把 src 传入 encoder，获得中间表达\n",
    "        context = self.encoder(src, src_lens)\n",
    "        # 把 context 输入 decoder， 得到预测结果\n",
    "        outs = self.decoder.infer(context, max_new_tokens=max_new_tokens)\n",
    "        return outs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760fe003-9e22-47da-bac1-7ad9faa43bab",
   "metadata": {},
   "source": [
    "### 7.训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b65b60ed-ff02-4d05-86a4-79dfa8164994",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bafcaa33-08cb-4406-8ca2-699be41794fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = Seq2Seq().to(device)\n",
    "\n",
    "# 加载上一次的模型\n",
    "last_model_path = os.path.join(\"models\", \"last.pt\")\n",
    "if os.path.exists(last_model_path):\n",
    "    print(\"加载 last.pt 模型\")\n",
    "    model.load_state_dict(state_dict=torch.load(f=last_model_path, weights_only=True, map_location=device))\n",
    "\n",
    "epochs = 15\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "96d09b41-0338-48b5-a4ac-fab9afc99b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    批量大小：32 个\n",
    "    显存占用：1,826 MiB\n",
    "    每轮耗时：90 S\n",
    "    \n",
    "\"\"\"\n",
    "def train():\n",
    "    if not os.path.exists(\"models\"):\n",
    "        os.mkdir(\"models\")\n",
    "    loss_file_path = os.path.join(\"models\", \"losses.csv\")\n",
    "    if os.path.exists(loss_file_path):\n",
    "        data = pd.read_csv(filepath_or_buffer=loss_file_path)\n",
    "        best_loss = data.loc[0, \"best_loss\"]\n",
    "        last_loss = data.loc[0, \"last_loss\"]\n",
    "    else:\n",
    "        best_loss = float(\"inf\")\n",
    "        last_loss = float(\"inf\")\n",
    "    print(f\"best_loss：{best_loss}\")\n",
    "    print(f\"last_loss：{last_loss}\")\n",
    "    early_stopping_count  = 0\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_epoch_losses = []\n",
    "        start_time = time.time()\n",
    "        for src, src_lens, tgt, tgt_lens in tqdm(train_dataloader):\n",
    "            # 数据搬家\n",
    "            src = src.to(device=device)\n",
    "            src_lens = src_lens\n",
    "            tgt = tgt.to(device=device)\n",
    "            tgt_lens = tgt_lens\n",
    "            # 1. 正向传播\n",
    "            outs = model(src, src_lens, tgt)\n",
    "            # 2， 损失计算（PAD也做了对齐，PAD也可以不用对齐）\n",
    "            tgt = tgt[1:, :].contiguous().view(-1)\n",
    "            outs = outs.contiguous().view(-1, outs.size(-1))\n",
    "            loss = loss_fn(outs, tgt)\n",
    "            # 3. 反向传播\n",
    "            loss.backward()\n",
    "            # 4. 优化一步\n",
    "            optimizer.step()\n",
    "            # 5. 清空梯度\n",
    "            optimizer.zero_grad()\n",
    "            # 6. 累积损失\n",
    "            running_epoch_losses.append(loss.item())\n",
    "        \n",
    "        # 每隔一轮一下损失\n",
    "        stop_time = time.time()\n",
    "        running_epoch_loss = sum(running_epoch_losses) / len(running_epoch_losses)\n",
    "        print(f\"Epoch: {epoch + 1} / {epochs} , train_loss: {running_epoch_loss}, time: {stop_time-start_time} S\")\n",
    "        # 观察结果\n",
    "        observe_the_result()\n",
    "        # 保存模型\n",
    "        if running_epoch_loss < last_loss:\n",
    "            last_loss = running_epoch_loss\n",
    "            best_loss = running_epoch_loss\n",
    "            # 保存模型\n",
    "            best_model_path = os.path.join(\"models\", \"best.pt\")\n",
    "            torch.save(obj=model.state_dict(), f=best_model_path)\n",
    "\n",
    "        # 保存 last.model\n",
    "        last_model_path = os.path.join(\"models\", \"last.pt\")\n",
    "        torch.save(obj=model.state_dict(), f=last_model_path)\n",
    "        # loss\n",
    "        loss_file_path = os.path.join(\"models\", \"losses.csv\")\n",
    "        last_loss = running_epoch_loss\n",
    "        data = pd.DataFrame(data={\"best_loss\":[best_loss], \"last_loss\": [last_loss]})\n",
    "        data.to_csv(path_or_buf=loss_file_path, index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4bb351-c558-4879-b204-4ef3b63b93d2",
   "metadata": {},
   "source": [
    "### 8. 模型推理\n",
    "- 输入：上句\n",
    "- 输出：下句"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8d168607-bc74-413f-bcb3-1869211037a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def observe_the_result(K = 3, dataloader=train_dataloader):\n",
    "    \"\"\"\n",
    "        在每轮训练结束后，抽取K个句子来观察效果\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for src, src_lens, tgt, tgt_lens in dataloader:\n",
    "            \n",
    "            # 做预测\n",
    "            y_preds = model.infer(src=src.to(device=device), src_lens=src_lens, max_new_tokens=128)\n",
    "            y_preds = y_preds.argmax(dim=-1).permute(dims=(1, 0)).cpu().numpy()\n",
    "            \n",
    "        \n",
    "            # 随机抽取 k 个句子做效果观察\n",
    "            _, batch_size = src.shape\n",
    "            samples_ids = random.sample(population=range(batch_size), k=K)\n",
    "            src = src[:, samples_ids]\n",
    "            tgt = tgt[:, samples_ids]\n",
    "            y_preds = y_preds[samples_ids, :]\n",
    "            \n",
    "            src_tokens = tokenizer.decode_src(src.permute(dims=(1, 0)).cpu().numpy())\n",
    "            tgt_tokens = tokenizer.decode_tgt(tgt_ids=tgt.permute(dims=(1, 0)).cpu().numpy())\n",
    "            y_preds = tokenizer.decode_tgt(tgt_ids=y_preds)\n",
    "        \n",
    "            for idx in range(K):\n",
    "                print(f\"# 第 {idx + 1} 句输入：{src_tokens[idx]}\")\n",
    "                print(f\"# 第 {idx + 1} 句标签：{tgt_tokens[idx]}\")\n",
    "                print(f\"# 第 {idx + 1} 句预测：{y_preds[idx]}\")\n",
    "                print(\"-\" * 80, \"\\n\")\n",
    "            \n",
    "            break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "39eee872-76e1-4bf8-ad4e-b8abd042620e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# observe_the_result()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4388be-f188-43ec-9b36-a140202d567b",
   "metadata": {},
   "source": [
    "# 执行训练\n",
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fd170c-7d08-4448-9567-57060897f952",
   "metadata": {},
   "source": [
    "### 9. 升级改造\n",
    "- 增加模型的复杂度：\n",
    "    - 把模型变成2层\n",
    "    - 双向（可选）\n",
    "- 增加句子的数量\n",
    "    - 几十万数量级\n",
    "    - 几百万数量级\n",
    "- 增加句子的长度：\n",
    "    - 几百个词左右\n",
    "    - 几千个词左右"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8811d2ed-399d-49af-8829-9c872187ca07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
