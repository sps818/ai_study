{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0b09f20-94dd-4ac6-b4af-4c25d994f2f8",
   "metadata": {},
   "source": [
    "### 1. 样本路径和类别读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad106c89-1433-4458-b62d-a9f75e85dcdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 5000)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    训练数据聚合\n",
    "\"\"\"\n",
    "import os\n",
    "train_root = os.path.join(\"hotel\", \"train\")\n",
    "train_texts = []\n",
    "train_labels = []\n",
    "for label in os.listdir(train_root):\n",
    "    label_root = os.path.join(train_root, label)\n",
    "    for file in os.listdir(label_root):\n",
    "        file_path = os.path.join(label_root, file)\n",
    "        # 聚合结果\n",
    "        train_texts.append(file_path)\n",
    "        train_labels.append(label)\n",
    "# 打印数据\n",
    "len(train_texts), len(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fb5d73f-55b7-44b5-97a0-51440c5e579b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1000)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    测试数据聚合\n",
    "\"\"\"\n",
    "test_root = os.path.join(\"hotel\", \"test\")\n",
    "test_texts = []\n",
    "test_labels = []\n",
    "for label in os.listdir(test_root):\n",
    "    label_root = os.path.join(test_root, label)\n",
    "    for file in os.listdir(label_root):\n",
    "        file_path = os.path.join(label_root, file)\n",
    "        # 聚合结果\n",
    "        test_texts.append(file_path)\n",
    "        test_labels.append(label)\n",
    "# 打印数据\n",
    "len(test_texts), len(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25896509-1a1f-4f43-abec-3521dc2b588b",
   "metadata": {},
   "source": [
    "### 2. 构建分词器\n",
    "- 分词，把句子变 token\n",
    "- 把所有不同的token聚在一起\n",
    "- 做 0 ~ N-1 的编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81e2f157-55cb-4f59-aa9e-a88e7c1c087b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e82aee1-5f49-409d-9496-0e70c01ae793",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "# pip install opencc -U\n",
    "import opencc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7501106a-9db2-47ec-9167-da5d2f8120d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer(object):\n",
    "    \"\"\"\n",
    "        定义一个分词器\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y):\n",
    "        \"\"\"\n",
    "            训练的语料\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.t2s = opencc.OpenCC(config=\"t2s\")\n",
    "        self._build_dict()\n",
    "\n",
    "    def _build_dict(self):\n",
    "        \"\"\"\n",
    "            构建字典\n",
    "        \"\"\"\n",
    "        # 1. 获取所有的 token\n",
    "        words = {\"<PAD>\", \"<UNK>\"}\n",
    "        for file in self.X:\n",
    "            # 1. 打开文件\n",
    "            with open(file=file, mode=\"r\", encoding=\"gbk\", errors=\"ignore\") as f:\n",
    "                text = f.read().replace(\"\\n\", \"\")\n",
    "                text = self.t2s.convert(text=text)\n",
    "                words.update(set(jieba.lcut(text)))\n",
    "        # 2. 构建文本字典\n",
    "        self.word2idx = {word: idx for idx, word in enumerate(words)}\n",
    "        self.idx2word = {idx: word for word, idx in self.word2idx.items()}\n",
    "        # 3. 删掉 数据集\n",
    "        del self.X\n",
    "        # 4. 构建标签字典\n",
    "        labels = set(train_labels)\n",
    "        self.label2idx = {label: idx for idx, label in enumerate(labels)}\n",
    "        self.idx2label = {idx: label for label, idx in self.label2idx.items()}\n",
    "        # 5. 删除 数据集\n",
    "        del self.y\n",
    "        \n",
    "\n",
    "    def encode(self, text, seq_len=SEQ_LEN):\n",
    "        \"\"\"\n",
    "            text --> tokens --> ids\n",
    "        \"\"\"\n",
    "        # 1. 繁体转简体\n",
    "        text = text.replace(\"\\n\", \"\")\n",
    "        text = self.t2s.convert(text=text)\n",
    "        # 2. 分词\n",
    "        text = jieba.lcut(text)\n",
    "        # 3. 统一长度\n",
    "        text = (text + [\"<PAD>\"] * seq_len)[:seq_len]\n",
    "        # 4. 转 id\n",
    "        ids = [self.word2idx.get(word, self.word2idx.get(\"<UNK>\")) for word in text]\n",
    "        \n",
    "        return ids\n",
    "        \n",
    "    def decode(self, ids):\n",
    "        \"\"\"\n",
    "            ids --> tokens --> text\n",
    "        \"\"\"\n",
    "        text = \"\".join([self.idx2word.get(_id, \"\") for _id in ids])\n",
    "        return text\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "            输: 分词器基本信息\n",
    "        \"\"\"\n",
    "        return f\"\"\"\n",
    "        Tokenizer Info: \n",
    "            --> Num of Tokens: {len(self.word2idx)}\n",
    "            --> Num of Labels: {len(self.label2idx)}\n",
    "        \"\"\"\n",
    "    def __repr__(self):\n",
    "        return self.__str__()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5d7590-05cb-4976-abc5-271efbfabd5d",
   "metadata": {},
   "source": [
    "### 3. 打包数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74c683fa-a60a-44c0-9169-aa9ea4e6bd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32834726-75af-46cc-bfe3-733348f19ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HotelCommentDataset(Dataset):\n",
    "    \"\"\"\n",
    "        自定义数据集\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, seq_len=SEQ_LEN):\n",
    "        \"\"\"\n",
    "            初始化\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "            索引操作\n",
    "                返回第idx个样本\n",
    "        \"\"\"\n",
    "        # 1. 文本\n",
    "        file = self.X[idx]\n",
    "        with open(file=file, mode=\"r\", encoding=\"gbk\", errors=\"ignore\") as f:\n",
    "            text = f.read()\n",
    "            ids = tokenizer.encode(text=text, seq_len=self.seq_len)\n",
    "            ids = torch.tensor(data=ids, dtype=torch.long)\n",
    "                \n",
    "        # 2. 标签\n",
    "        label = self.y[idx]\n",
    "        label = tokenizer.label2idx.get(label)\n",
    "        label = torch.tensor(data=label, dtype=torch.long)\n",
    "        \n",
    "        return ids, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff9e0525-bfa9-411f-9791-261eab463dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\63447\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.366 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "        Tokenizer Info: \n",
       "            --> Num of Tokens: 20781\n",
       "            --> Num of Labels: 2\n",
       "        "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. 定义一个分词器\n",
    "tokenizer = Tokenizer(X=train_texts, y=train_labels)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27aa23f9-9ab2-4911-bf22-536619a860a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 打包数据\n",
    "train_dataset = HotelCommentDataset(X=train_texts, y=train_labels)\n",
    "train_dataloader = DataLoader(dataset=train_dataset, shuffle=True, batch_size=128)\n",
    "test_dataset = HotelCommentDataset(X=test_texts, y=test_labels)\n",
    "test_dataloader = DataLoader(dataset=test_dataset, shuffle=False, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbbb2a22-f816-4a78-ba56-3f25d3fbb609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 128])\n",
      "torch.Size([256])\n"
     ]
    }
   ],
   "source": [
    "for X, y in test_dataloader:\n",
    "    print(X.shape)\n",
    "    print(y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405343ba-dc1c-47ac-a35f-1bfbb2216fea",
   "metadata": {},
   "source": [
    "### 4. 搭建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e79c9060-2b82-4ed3-b0fc-1b20da091c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "778471e4-a16c-4b37-9b69-10b02625ff56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    每句话65个词，分为2类\\n\\n        - 解决？\\n    \\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    每句话65个词，分为2类\n",
    "\n",
    "        - 解决？\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a68ec2c4-438e-49ad-9432-f55dfdfcff56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    \"\"\"\n",
    "        搭建模型\n",
    "            - 卷积？\n",
    "            # [N, C, L]\n",
    "            nn.Conv1d()\n",
    "    \"\"\"\n",
    "    def __init__(self, dict_len=len(tokenizer.word2idx), embedding_dim = 256):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(num_embeddings=dict_len, \n",
    "                                  embedding_dim=embedding_dim, \n",
    "                                  padding_idx=tokenizer.word2idx.get(\"<PAD>\"))\n",
    "        self.conv1 = nn.Conv1d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(num_features=512)\n",
    "        self.mp1 = nn.MaxPool1d(kernel_size=2, stride=2, padding=0)\n",
    "        self.conv2 = nn.Conv1d(in_channels=512, out_channels=1024, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(num_features=1024)\n",
    "        self.mp2 = nn.MaxPool1d(kernel_size=2, stride=2, padding=0)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(in_features=1024 *(SEQ_LEN // 2 // 2), out_features=128)\n",
    "        self.fc2 = nn.Linear(in_features=128, out_features=2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        x = torch.permute(input=x, dims=(0, 2, 1))\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.mp1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.mp2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd22fedc-8d51-493b-9292-5b9bcf0558fe",
   "metadata": {},
   "source": [
    "### 5. 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6479dfdc-7fa5-40f3-a39b-f6caf18fd43d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9236aa81-3315-4d23-b3f7-cee285d4ea5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检测设备\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# 实例化模型\n",
    "model = Model().to(device=device)\n",
    "# 损失函数\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# 优化器\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-3)\n",
    "# 轮次\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30651c65-8ba2-42f3-8a94-a6fb9749f150",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    评估\n",
    "\"\"\"\n",
    "def get_acc(dataloader):\n",
    "    model.eval()\n",
    "    accs = []\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            # 0. 数据搬家\n",
    "            X = X.to(device=device)\n",
    "            y = y.to(device=device)\n",
    "            # 1. 正向传播\n",
    "            y_pred = model(X)\n",
    "            # 2. 计算结果\n",
    "            y_pred = y_pred.argmax(dim=1)\n",
    "            # 3. 计算准确率\n",
    "            acc = (y_pred == y).to(dtype=torch.float32).mean().item()\n",
    "            # 4. 保存结果\n",
    "            accs.append(acc)\n",
    "    final_acc = round(number=sum(accs) / len(accs), ndigits=6)\n",
    "    return final_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "750c0284-08d1-4db3-bb78-615f319dbd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    train_acc = get_acc(dataloader=train_dataloader)\n",
    "    test_acc = get_acc(dataloader=test_dataloader)\n",
    "    print(f\"初始 Train_acc: {train_acc}, Test_acc: {test_acc}\")\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for X, y in train_dataloader:\n",
    "            # 0. 数据搬家\n",
    "            X = X.to(device=device)\n",
    "            y = y.to(device=device)\n",
    "            # 1. 正向传播\n",
    "            y_pred = model(X)\n",
    "            # 2. 计算误差\n",
    "            loss = loss_fn(y_pred, y)\n",
    "            # 3. 反向传播\n",
    "            loss.backward()\n",
    "            # 4. 优化一步\n",
    "            optimizer.step()\n",
    "            # 5. 清空梯度\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        # 每轮结束后测试一下\n",
    "        train_acc = get_acc(dataloader=train_dataloader)\n",
    "        test_acc = get_acc(dataloader=test_dataloader)\n",
    "        \n",
    "        print(f\"Epoch: {epoch + 1}, Train_acc: {train_acc}, Test_acc: {test_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b2931e61-357a-498b-91ce-a03517f72af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初始 Train_acc: 0.504687, Test_acc: 0.513672\n",
      "Epoch: 1, Train_acc: 0.73125, Test_acc: 0.67669\n",
      "Epoch: 2, Train_acc: 0.911133, Test_acc: 0.830078\n",
      "Epoch: 3, Train_acc: 0.966211, Test_acc: 0.826239\n",
      "Epoch: 4, Train_acc: 0.994531, Test_acc: 0.836712\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[16], line 23\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# 每轮结束后测试一下\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mget_acc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m test_acc \u001b[38;5;241m=\u001b[39m get_acc(dataloader\u001b[38;5;241m=\u001b[39mtest_dataloader)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train_acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Test_acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_acc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[15], line 8\u001b[0m, in \u001b[0;36mget_acc\u001b[1;34m(dataloader)\u001b[0m\n\u001b[0;32m      6\u001b[0m accs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m----> 8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m X, y \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[0;32m      9\u001b[0m         \u001b[38;5;66;03m# 0. 数据搬家\u001b[39;00m\n\u001b[0;32m     10\u001b[0m         X \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m     11\u001b[0m         y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[1;32mD:\\software\\Anaconda\\envs\\py39_normal\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mD:\\software\\Anaconda\\envs\\py39_normal\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mD:\\software\\Anaconda\\envs\\py39_normal\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mD:\\software\\Anaconda\\envs\\py39_normal\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[7], line 23\u001b[0m, in \u001b[0;36mHotelCommentDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     21\u001b[0m     text \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m     22\u001b[0m     ids \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(text\u001b[38;5;241m=\u001b[39mtext, seq_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseq_len)\n\u001b[1;32m---> 23\u001b[0m     ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(data\u001b[38;5;241m=\u001b[39mids, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# 2. 标签\u001b[39;00m\n\u001b[0;32m     26\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my[idx]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ce613e-a56a-404e-ba8f-215061cfa874",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f5a618-5f76-4379-9ca6-7d529b1a41e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33dd1a2-3a86-4ad5-a9c2-16cb2fdd67f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad1c211-d777-4de1-aee3-f7bd4091241c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03745b0f-afe3-493a-a36f-d8ca161b1dd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
