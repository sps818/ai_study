{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0b09f20-94dd-4ac6-b4af-4c25d994f2f8",
   "metadata": {},
   "source": [
    "### 1. 样本路径和类别读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad106c89-1433-4458-b62d-a9f75e85dcdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 5000)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    训练数据聚合\n",
    "\"\"\"\n",
    "import os\n",
    "train_root = os.path.join(\"hotel\", \"train\")\n",
    "train_texts = []\n",
    "train_labels = []\n",
    "for label in os.listdir(train_root):\n",
    "    label_root = os.path.join(train_root, label)\n",
    "    for file in os.listdir(label_root):\n",
    "        file_path = os.path.join(label_root, file)\n",
    "        # 聚合结果\n",
    "        train_texts.append(file_path)\n",
    "        train_labels.append(label)\n",
    "# 打印数据\n",
    "len(train_texts), len(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fb5d73f-55b7-44b5-97a0-51440c5e579b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1000)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    测试数据聚合\n",
    "\"\"\"\n",
    "test_root = os.path.join(\"hotel\", \"test\")\n",
    "test_texts = []\n",
    "test_labels = []\n",
    "for label in os.listdir(test_root):\n",
    "    label_root = os.path.join(test_root, label)\n",
    "    for file in os.listdir(label_root):\n",
    "        file_path = os.path.join(label_root, file)\n",
    "        # 聚合结果\n",
    "        test_texts.append(file_path)\n",
    "        test_labels.append(label)\n",
    "# 打印数据\n",
    "len(test_texts), len(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25896509-1a1f-4f43-abec-3521dc2b588b",
   "metadata": {},
   "source": [
    "### 2. 构建分词器\n",
    "- 分词，把句子变 token\n",
    "- 把所有不同的token聚在一起\n",
    "- 做 0 ~ N-1 的编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81e2f157-55cb-4f59-aa9e-a88e7c1c087b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e82aee1-5f49-409d-9496-0e70c01ae793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencc in d:\\software\\anaconda\\envs\\py39_normal\\lib\\site-packages (1.1.9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -upyterlab (d:\\software\\anaconda\\envs\\py39_normal\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -upyterlab (d:\\software\\anaconda\\envs\\py39_normal\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -upyterlab (d:\\software\\anaconda\\envs\\py39_normal\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "!pip install opencc -U\n",
    "import opencc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7501106a-9db2-47ec-9167-da5d2f8120d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer(object):\n",
    "    \"\"\"\n",
    "        定义一个分词器\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y):\n",
    "        \"\"\"\n",
    "            训练的语料\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.t2s = opencc.OpenCC(config=\"t2s\")\n",
    "        self._build_dict()\n",
    "\n",
    "    def _build_dict(self):\n",
    "        \"\"\"\n",
    "            构建字典\n",
    "        \"\"\"\n",
    "        # 1. 获取所有的 token\n",
    "        words = {\"<PAD>\", \"<UNK>\"}\n",
    "        for file in self.X:\n",
    "            # 1. 打开文件\n",
    "            with open(file=file, mode=\"r\", encoding=\"gbk\", errors=\"ignore\") as f:\n",
    "                text = f.read().replace(\"\\n\", \"\")\n",
    "                text = self.t2s.convert(text=text)\n",
    "                words.update(set(jieba.lcut(text)))\n",
    "        # 2. 构建文本字典\n",
    "        self.word2idx = {word: idx for idx, word in enumerate(words)}\n",
    "        self.idx2word = {idx: word for word, idx in self.word2idx.items()}\n",
    "        # 3. 删掉 数据集\n",
    "        del self.X\n",
    "        # 4. 构建标签字典\n",
    "        labels = set(train_labels)\n",
    "        self.label2idx = {label: idx for idx, label in enumerate(labels)}\n",
    "        self.idx2label = {idx: label for label, idx in self.label2idx.items()}\n",
    "        # 5. 删除 数据集\n",
    "        del self.y\n",
    "        \n",
    "\n",
    "    def encode(self, text, seq_len=SEQ_LEN):\n",
    "        \"\"\"\n",
    "            text --> tokens --> ids\n",
    "\n",
    "            自我扩展:\n",
    "                - 右侧截断或填充\n",
    "                - 左边？\n",
    "                - 随机？\n",
    "        \"\"\"\n",
    "        # 1. 繁体转简体\n",
    "        text = text.replace(\"\\n\", \"\")\n",
    "        text = self.t2s.convert(text=text)\n",
    "        # 2. 分词\n",
    "        text = jieba.lcut(text)\n",
    "        # 3. 统一长度\n",
    "        text = (text + [\"<PAD>\"] * seq_len)[:seq_len]\n",
    "        # 4. 转 id\n",
    "        ids = [self.word2idx.get(word, self.word2idx.get(\"<UNK>\")) for word in text]\n",
    "        \n",
    "        return ids\n",
    "        \n",
    "    def decode(self, ids):\n",
    "        \"\"\"\n",
    "            ids --> tokens --> text\n",
    "        \"\"\"\n",
    "        text = \"\".join([self.idx2word.get(_id, \"\") for _id in ids])\n",
    "        return text\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "            输: 分词器基本信息\n",
    "        \"\"\"\n",
    "        return f\"\"\"\n",
    "        Tokenizer Info: \n",
    "            --> Num of Tokens: {len(self.word2idx)}\n",
    "            --> Num of Labels: {len(self.label2idx)}\n",
    "        \"\"\"\n",
    "    def __repr__(self):\n",
    "        return self.__str__()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5d7590-05cb-4976-abc5-271efbfabd5d",
   "metadata": {},
   "source": [
    "### 3. 打包数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74c683fa-a60a-44c0-9169-aa9ea4e6bd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32834726-75af-46cc-bfe3-733348f19ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HotelCommentDataset(Dataset):\n",
    "    \"\"\"\n",
    "        自定义数据集\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, seq_len=SEQ_LEN):\n",
    "        \"\"\"\n",
    "            初始化\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "            索引操作\n",
    "                返回第idx个样本\n",
    "        \"\"\"\n",
    "        # 1. 文本\n",
    "        file = self.X[idx]\n",
    "        with open(file=file, mode=\"r\", encoding=\"gbk\", errors=\"ignore\") as f:\n",
    "            text = f.read()\n",
    "            ids = tokenizer.encode(text=text, seq_len=self.seq_len)\n",
    "            ids = torch.tensor(data=ids, dtype=torch.long)\n",
    "                \n",
    "        # 2. 标签\n",
    "        label = self.y[idx]\n",
    "        label = tokenizer.label2idx.get(label)\n",
    "        label = torch.tensor(data=label, dtype=torch.long)\n",
    "        \n",
    "        return ids, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff9e0525-bfa9-411f-9791-261eab463dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\63447\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.440 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "        Tokenizer Info: \n",
       "            --> Num of Tokens: 20781\n",
       "            --> Num of Labels: 2\n",
       "        "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. 定义一个分词器\n",
    "tokenizer = Tokenizer(X=train_texts, y=train_labels)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27aa23f9-9ab2-4911-bf22-536619a860a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 打包数据\n",
    "train_dataset = HotelCommentDataset(X=train_texts, y=train_labels)\n",
    "train_dataloader = DataLoader(dataset=train_dataset, shuffle=True, batch_size=128)\n",
    "test_dataset = HotelCommentDataset(X=test_texts, y=test_labels)\n",
    "test_dataloader = DataLoader(dataset=test_dataset, shuffle=False, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbbb2a22-f816-4a78-ba56-3f25d3fbb609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 85])\n",
      "torch.Size([256])\n"
     ]
    }
   ],
   "source": [
    "for X, y in test_dataloader:\n",
    "    print(X.shape)\n",
    "    print(y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405343ba-dc1c-47ac-a35f-1bfbb2216fea",
   "metadata": {},
   "source": [
    "### 4. 搭建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e79c9060-2b82-4ed3-b0fc-1b20da091c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "778471e4-a16c-4b37-9b69-10b02625ff56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    每句话65个词，分为2类\\n\\n        - 解决？\\n    \\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    每句话65个词，分为2类\n",
    "\n",
    "        - 解决？\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a68ec2c4-438e-49ad-9432-f55dfdfcff56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN(nn.Module):\n",
    "    \"\"\"\n",
    "        搭建模型\n",
    "            - 卷积？\n",
    "            # [N, C, L]\n",
    "            nn.Conv1d()\n",
    "    \"\"\"\n",
    "    def __init__(self, dict_len=len(tokenizer.word2idx), embedding_dim = 256):\n",
    "        super().__init__()\n",
    "        # 向量化\n",
    "        self.embed = nn.Embedding(num_embeddings=dict_len, \n",
    "                                  embedding_dim=embedding_dim, \n",
    "                                  padding_idx=tokenizer.word2idx.get(\"<PAD>\"))\n",
    "        # 特征抽取\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm1d(num_features=512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2, padding=0),\n",
    "            nn.Conv1d(in_channels=512, out_channels=1024, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm1d(num_features=1024),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2, padding=0)\n",
    "        )\n",
    "        # 分类\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=1024 * (SEQ_LEN // 2 // 2), out_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=128, out_features=2)\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 向量化\n",
    "        x = self.embed(x)\n",
    "        x = torch.permute(input=x, dims=(0, 2, 1))\n",
    "        # 抽特征\n",
    "        x = self.feature_extractor(x)\n",
    "        # 做分类\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8cca24f-eb5b-4b4b-a140-3524c81f0770",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextRNN(nn.Module):\n",
    "    \"\"\"\n",
    "        搭建模型\n",
    "            - 卷积？\n",
    "            # [N, C, L]\n",
    "            nn.Conv1d()\n",
    "    \"\"\"\n",
    "    def __init__(self, dict_len=len(tokenizer.word2idx), embedding_dim = 256):\n",
    "        super().__init__()\n",
    "        # 向量化\n",
    "        self.embed = nn.Embedding(num_embeddings=dict_len, \n",
    "                                  embedding_dim=embedding_dim, \n",
    "                                  padding_idx=tokenizer.word2idx.get(\"<PAD>\"))\n",
    "        # 特征抽取\n",
    "        self.feature_extractor = nn.RNN(input_size=256, hidden_size=512, num_layers=1, bidirectional=False)\n",
    "        # 分类\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(in_features=512, out_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=128, out_features=2)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 向量化 [batch_size, seq_len]\n",
    "        x = self.embed(x)\n",
    "        # [batch_size, seq_len, embedding_dim]\n",
    "        x = torch.permute(input=x, dims=(1, 0, 2))\n",
    "        # 特征抽取 [seq_len, batch_size, embedding_dim]\n",
    "        out, hn = self.feature_extractor(x)\n",
    "        # 分类输出\n",
    "        x = self.classifier(out.sum(dim=0))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a1286ae-8a6d-4866-b7c6-f278042fea0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 2])\n"
     ]
    }
   ],
   "source": [
    "model= TextRNN()\n",
    "for X, y in train_dataloader:\n",
    "    y_pred = model(X)\n",
    "    print(y_pred.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd22fedc-8d51-493b-9292-5b9bcf0558fe",
   "metadata": {},
   "source": [
    "### 5. 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9236aa81-3315-4d23-b3f7-cee285d4ea5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检测设备\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# 实例化模型\n",
    "model = TextRNN().to(device=device)\n",
    "# 损失函数\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# 优化器\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-3)\n",
    "# 轮次\n",
    "epochs = 20\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "30651c65-8ba2-42f3-8a94-a6fb9749f150",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    评估\n",
    "\"\"\"\n",
    "def get_acc(dataloader):\n",
    "    model.eval()\n",
    "    accs = []\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            # 0. 数据搬家\n",
    "            X = X.to(device=device)\n",
    "            y = y.to(device=device)\n",
    "            # 1. 正向传播\n",
    "            y_pred = model(X)\n",
    "            # 2. 计算结果\n",
    "            y_pred = y_pred.argmax(dim=1)\n",
    "            # 3. 计算准确率\n",
    "            acc = (y_pred == y).to(dtype=torch.float32).mean().item()\n",
    "            # 4. 保存结果\n",
    "            accs.append(acc)\n",
    "    final_acc = round(number=sum(accs) / len(accs), ndigits=6)\n",
    "    return final_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "750c0284-08d1-4db3-bb78-615f319dbd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    train_acc = get_acc(dataloader=train_dataloader)\n",
    "    test_acc = get_acc(dataloader=test_dataloader)\n",
    "    print(f\"初始 Train_acc: {train_acc}, Test_acc: {test_acc}\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        start_time = time.time()\n",
    "        for X, y in train_dataloader:\n",
    "            # 0. 数据搬家\n",
    "            X = X.to(device=device)\n",
    "            y = y.to(device=device)\n",
    "            # 1. 正向传播\n",
    "            y_pred = model(X)\n",
    "            # 2. 计算误差\n",
    "            loss = loss_fn(y_pred, y)\n",
    "            # 3. 反向传播\n",
    "            loss.backward()\n",
    "            # 4. 优化一步\n",
    "            optimizer.step()\n",
    "            # 5. 清空梯度\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        stop_time = time.time()\n",
    "        # 每轮结束后测试一下\n",
    "        train_acc = get_acc(dataloader=train_dataloader)\n",
    "        test_acc = get_acc(dataloader=test_dataloader)\n",
    "        \n",
    "        print(f\"Epoch: {epoch + 1}, Train_acc: {train_acc}, Test_acc: {test_acc}, Train_time: {stop_time-start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b2931e61-357a-498b-91ce-a03517f72af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初始 Train_acc: 0.524023, Test_acc: 0.535123\n",
      "Epoch: 1, Train_acc: 0.813867, Test_acc: 0.716056, Train_time: 8.453396797180176\n",
      "Epoch: 2, Train_acc: 0.87207, Test_acc: 0.793777, Train_time: 8.43115758895874\n",
      "Epoch: 3, Train_acc: 0.930469, Test_acc: 0.78647, Train_time: 8.381408929824829\n",
      "Epoch: 4, Train_acc: 0.90332, Test_acc: 0.788254, Train_time: 8.417727708816528\n",
      "Epoch: 5, Train_acc: 0.963672, Test_acc: 0.803509, Train_time: 8.386383056640625\n",
      "Epoch: 6, Train_acc: 0.981445, Test_acc: 0.80489, Train_time: 8.476622343063354\n",
      "Epoch: 7, Train_acc: 0.984766, Test_acc: 0.815228, Train_time: 8.406984329223633\n",
      "Epoch: 8, Train_acc: 0.984766, Test_acc: 0.793204, Train_time: 8.861475229263306\n",
      "Epoch: 9, Train_acc: 0.986914, Test_acc: 0.815564, Train_time: 8.708907127380371\n",
      "Epoch: 10, Train_acc: 0.998047, Test_acc: 0.815901, Train_time: 8.603615045547485\n",
      "Epoch: 11, Train_acc: 0.968359, Test_acc: 0.800815, Train_time: 8.52812123298645\n",
      "Epoch: 12, Train_acc: 0.958398, Test_acc: 0.816272, Train_time: 8.63102126121521\n",
      "Epoch: 13, Train_acc: 0.992969, Test_acc: 0.827957, Train_time: 8.685420751571655\n",
      "Epoch: 14, Train_acc: 0.996484, Test_acc: 0.816777, Train_time: 8.572360515594482\n",
      "Epoch: 15, Train_acc: 0.999023, Test_acc: 0.808863, Train_time: 8.598994970321655\n",
      "Epoch: 16, Train_acc: 0.980273, Test_acc: 0.795393, Train_time: 31635.996307849884\n",
      "Epoch: 17, Train_acc: 0.986719, Test_acc: 0.816204, Train_time: 9.243302345275879\n",
      "Epoch: 18, Train_acc: 0.960938, Test_acc: 0.785594, Train_time: 8.736217021942139\n",
      "Epoch: 19, Train_acc: 0.991602, Test_acc: 0.817282, Train_time: 8.35993242263794\n",
      "Epoch: 20, Train_acc: 0.996094, Test_acc: 0.816339, Train_time: 8.310228109359741\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ce613e-a56a-404e-ba8f-215061cfa874",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
