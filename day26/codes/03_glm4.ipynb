{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89a13deb-244a-46ce-8287-072bd3e52148",
   "metadata": {},
   "source": [
    "### 1. 引入依赖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7863fa5b-057c-461c-9599-b3ba42ef840c",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PyTorch 框架\n",
    "import torch\n",
    "# 模型加载器\n",
    "from transformers import AutoModelForCausalLM\n",
    "# 分词器加载器\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "123f9ddf-8ec8-46c0-9be3-a69c0d00fec6",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 模型地址\n",
    "# chat = instruct\n",
    "model_dir = \"./glm-4-9b-chat/\"\n",
    "# GPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4950ff8-b754-4326-9fa9-63c756a52f56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105d4314-b6ae-4e1f-af93-d7adec8fbf76",
   "metadata": {},
   "source": [
    "### 2. 加载分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd456bad-7cbd-470d-928f-4f451c373001",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_dir,\n",
    "                                          trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b7516a-9db3-4811-80b4-77a1705ae14f",
   "metadata": {},
   "source": [
    "### 3. 加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5c293d7-223a-4b78-81b5-f62823bc0c68",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d9ef726867240698c9ad4fc4e341f7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=model_dir,\n",
    "                                             torch_dtype=torch.bfloat16,\n",
    "                                             low_cpu_mem_usage=True,\n",
    "                                             trust_remote_code=True,\n",
    "                                             device_map=\"auto\"\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a68c833-e37b-4564-a462-0693b3a4161f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGLMForConditionalGeneration(\n",
       "  (transformer): ChatGLMModel(\n",
       "    (embedding): Embedding(\n",
       "      (word_embeddings): Embedding(151552, 4096)\n",
       "    )\n",
       "    (rotary_pos_emb): RotaryEmbedding()\n",
       "    (encoder): GLMTransformer(\n",
       "      (layers): ModuleList(\n",
       "        (0-39): 40 x GLMBlock(\n",
       "          (input_layernorm): RMSNorm()\n",
       "          (self_attention): SelfAttention(\n",
       "            (query_key_value): Linear(in_features=4096, out_features=4608, bias=True)\n",
       "            (core_attention): SdpaAttention(\n",
       "              (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (dense): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "          (post_attention_layernorm): RMSNorm()\n",
       "          (mlp): MLP(\n",
       "            (dense_h_to_4h): Linear(in_features=4096, out_features=27392, bias=False)\n",
       "            (dense_4h_to_h): Linear(in_features=13696, out_features=4096, bias=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_layernorm): RMSNorm()\n",
       "    )\n",
       "    (output_layer): Linear(in_features=4096, out_features=151552, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dd3f64-1abc-4fc3-aa62-953257136614",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "118e1f8e-a6eb-429a-bebb-7e08b9aea147",
   "metadata": {},
   "source": [
    "### 4. 准备提问"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8c5f713-341f-4234-bb82-f10724f5ef75",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 方便编程\n",
    "messages = [{\"role\":\"system\", \"content\":\"You are a helpful assistant!\"},\n",
    "           {\"role\":\"user\", \"content\":\"你是谁？\"}]\n",
    "# 格式化成文本\n",
    "text = tokenizer.apply_chat_template(conversation=messages,\n",
    "                                    add_generation_prompt=True,\n",
    "                                    tokenize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ff1d8bc-9338-4325-b85a-1da76127f971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[gMASK]<sop><|system|>\n",
      "You are a helpful assistant!<|user|>\n",
      "你是谁？<|assistant|>\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9ac6037-158a-4f58-a488-daa7c058ea3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  return_tensors=\"pt\" 意思是返回一个pytorch\n",
    "inputs = tokenizer(text=[text], return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62aa5771-4068-4a6d-9ac6-08d17d97db6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[151331, 151333, 151331, 151333, 151335,    198,   2610,    525,    264,\n",
       "          10945,  17821,      0, 151336,    198, 103408,  99668,  11314, 151337]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'position_ids': tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17]])}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12666fd5-1c67-4090-9cf1-7fa93c30d454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 推理过程需要的输入\n",
    "input_ids = inputs[\"input_ids\"].to(device=device)\n",
    "attention_mask = inputs[\"attention_mask\"].to(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818e9a53-60ed-48e3-ace3-d32970c29de1",
   "metadata": {},
   "source": [
    "### 5. 生成答案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69af682d-4de1-4435-88f5-901efee40f1e",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[gMASK] <sop> [gMASK] <sop> <|system|> \n",
      "You are a helpful assistant! <|user|> \n",
      "你是谁？ <|assistant|> \n",
      "我是一个名为 ChatGLM 的人工智能助手，是基于清华大学 KEG 实验室和智谱 AI 公司于 2024 年共同训练的语言模型开发的。我的任务是针对用户的问题和要求提供适当的答复和支持。 <|user|>\n"
     ]
    }
   ],
   "source": [
    "# 设为 评估模式 dropout\n",
    "model.eval()\n",
    "\n",
    "gen_kwargs = {\"max_length\": 2500, \"do_sample\": True, \"top_k\": 1, \"temperature\": 0.1}\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(input_ids =input_ids,\n",
    "                             attention_mask=attention_mask,\n",
    "                             **gen_kwargs)\n",
    "    # outputs = outputs[:, inputs['input_ids'].shape[1]:]\n",
    "    print(tokenizer.decode(outputs[0], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33b032e-4fe9-4d2f-bc40-5fd437762e20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(py311)",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
