### 学习的重点：
- 1. 模型的训练/微调
- 2. 部署模型
- 3. 上层开发（以LangChain家族为核心）
  - 1. Open API  
  - 2. Prompt 开发
  - 3. RAG 开发
  - 4. Agent 开发

### 训练 train 和 微调 fine-tune

- 训练：
  - 从零开始，训练一个大模型
  - 买一块布，手动做一条裤子
  - 数据量：
    - 预训练 18T
  - 时间：
    - 2个月
  - 训练平台：
    - 千卡
    - 万卡
    - H100
    - A100
  
- 微调：
  - 从别人训练好的开源大模型开始，去做一个具体任务微调
  - 买一条成品裤，你根据腿长剪裤脚
  - 数据量：
    - 几十条起
  - 时间：
    - 几分钟起
  - 训练平台：
    - 能把模型放下即可
    - 4090
- 共同点：
  - 都是修改模型的参数

- 三阶段：
  - 第1阶段：预训练
    - Pre-Train
    - 内功修炼
    - 自监督
      - 无需标注
    - 自回归方式训练
    - 数据：
      - 一段一段的文本
    - 成果
      - base 大模型
      - 半成品，不是直接用来做任务，而是让下游任务微调的！
      - 只能进行简单的文本续写（不具备质量遵循和函数调用等能力）

  - 第2阶段：监督指令微调
    - Supervised Finetune
    - 对标业务
    - 对标具体的任务
    - 外功修炼篇
    - 功能：
      - 1. 对标人类对话（什么梗都接得住，万金油）
      - 2. 指令遵循能力（你让他干什么，他就干什么）
      - 3. 函数调用能力（大模型能够判断是否时候需要借助外部函数）
      - 4. 复杂推理能力（大模型可以分步骤拆解推理和推理）
      - ...
    - 数据(知识编辑)：
      - 问答对 question answer pair
      - system
      - user
      - assistant
      - function_call
      - history
      - ...
    - 两种风格：
      - 为了注入知识
      - 为了提升能力
    - 并行训练


  - 第3阶段：偏好优化
    - RLHF: Reinforcement Learning from Human Feedback
    - DPO: Direct Preference Optimization
    - 一问两答：
      - 一问：
        - 一个问题
      - 两答：
        - 一个不好的答案：
          - 大模型当前的回答
        - 一个好的答案：
          - 你修改之后的答案
    - 产物：
      - chat
      - instruct

- 全程零代码
- 工程平衡：
  - 重点突出我们微调能力
  - 但是，不能太大的破坏原有的能力！！！

### 模型的测评
- 微调：
  - 废了是常态
  - 训练好是一件不太容易的事情
    - 没有万能公式，不断炼丹尝试的过程，比较玄学
- 考试：
  - 全学科考试
    - MMLU
    - CMMLU
    - C-Eval
    - ...